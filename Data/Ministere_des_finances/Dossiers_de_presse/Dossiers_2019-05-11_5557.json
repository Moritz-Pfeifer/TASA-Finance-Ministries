{"id": 5557, "url": "https://minefi.hosting.augure.com/Augure_Minefi/r/ContenuEnLigne/Download?id=AE5B7ED5-2385-4749-9CE8-E4E1B36873E4&filename=Mission%20Re%CC%81gulation%20des%20re%CC%81seaux%20sociaux%20-ENG.pdf", "author": null, "title": "18 - Rapport de la mission de régulation des réseaux sociaux - Version anglaise - Mai 2019", "text": " \n \n \n \n \n \n \n \nCreating a French framework  \n \nto make social media platforms more accountable:  \n \nActing in France with a European vision \n \n \n \n \n \n \n \n \n \n \n \n \nInterim mission report \n \n \n \n \n“Regulation of social networks – Facebook experiment” \n \n \n \n \n Submitted to the French Secretary of State for Digital Affairs \n \n \n \n \n \n \nMay 2019 \n \n \n\np. 2 \nExecutive Summary \n \nSocial networks allow any member of society to publish any content they wish and share it with other \nusers of the network. They are thereby revolutionising the media industry and communications by \noffering individuals and civil society a direct means of expression. It is no longer necessary to use \nconventional media to communicate publicly. Using social networks therefore considerably increases \nindividuals’ ability to exercise their freedom of expression, communicate and obtain information. \nNevertheless, the opportunities offered by social networking services can lead to unacceptable abuses of \nthose same freedoms. These abuses are being committed by isolated individuals or organised groups to \nwhich the leading social networks – including Facebook, YouTube, Twitter and Snapchat, to cite just the \nlargest – are not providing an adequate response. Yet through their ordering of published content and \nmoderation policies, social networks have the ability to take direct action against the worst abuses to \nprevent or respond to them and thereby limit the damage to social cohesion. \nPublic intervention to force the biggest players to assume a more responsible and protective attitude to \nour social cohesion therefore appears legitimate. Given the civil liberty issues at stake, this intervention \nshould be subject to particular precautions. It must (1) respect the wide range of social network models, \nwhich are particularly diverse, (2) impose a principle of transparency and systematic inclusion of civil \nsociety, (3) aim for a minimum level of intervention in accordance with the principles of necessity and \nproportionality and (4) refer to the courts for the characterisation of the lawfulness of individual content. \nThe current approach of self-regulation of social networks is interesting, as it demonstrates that platforms \nmay be part of the solution to the problems observed. They have come up with varied and agile solutions, \ne.g. removal, less exposure, reminder of common rules, education and victim support. But self-regulation \nis still evolving, remains too reactive (after the appearance of harm), and lacks credibility due to the \nextreme asymmetry of information, which gives rise to a sense of “story-telling” which nourishes \nsuspicion about the reality of the platform’s actions. \nThe public policy response must find a balance between a punitive approach, which is vital for sending a \nstrong political signal to the perpetrators of abuses, and the approach of making social networks \nincreasingly accountable through preventive regulation, capitalising on platforms’ capacity for self-\nregulation. \nGiven the unique and ubiquitous nature of social networks, which transcend the borders of Member \nStates and offer a unique service in different areas, this ex-ante regulation must be adopted and \nimplemented at European level. The current “installation country” approach – according to which only \nthe country in which the social network’s headquarters is based can intervene to regulate this network – \nhas proven inefficient. The damage caused by the excesses and abuses of social networks to social \ncohesion in destination Member States is difficult to observe from the installation Member State.  \nAny French initiative should therefore aim to reverse the current European approach to focus instead on \nthe destination country, in which the platform is responsible to the Member State where the damage has \noccurred. This would strengthen each Member State’s ability to address the consequences of globalisation. \nThis objective must be taken into account when designing a regulatory function for social networks so \nthat the solution appears relevant for our main European partners, even though the policy on regulating \nmedia industries differs significantly from one state to another.  \nThe implementation of an ex-ante regulatory function should respect three conditions: (1) to adopt a \ncompliance approach, according to which the regulator supervises the correct implementation of \npreventive or corrective measures, but does not focus on the materialisation of risks nor try to regulate the \nservice provided, (2) to concentrate on the systemic actors capable of creating significant damages to our \nsocieties, without creating entry barriers for new European operators, (3) to stay agile to confront future \n\n \n \n \n \np. 3 \nchallenges in a rapidly evolving digital environment. Legislative measures should therefore aim to create an \ninstitutional capacity to regulate rather than a regulation specifically applicable to current problems. \nThat regulation could be based on the following five pillars: \nFirst pillar: \nA public regulatory policy guaranteeing individual freedoms and platforms’ \nentrepreneurial freedom. \nSecond pillar: \nA prescriptive regulation focusing on the accountability of social networks, \nimplemented by an independent administrative authority and based on \nthree obligations for the platforms: \n \nObligation of transparency of the function of ordering content, \n \nObligation of transparency of the function which implements the Terms \nof Service and the moderation of content, \n \nObligation to defend the integrity of its users. \n \nThird pillar:  \nInformed political dialogue between the operators, the government, the \nlegislature and civil society. \nFourth pillar:  \nAn independent administrative authority, acting in partnership with other \nbranches of the state, and open to civil society. \nFifth pillar:  \nA European cooperation, reinforcing Member States’ capacity to act against \nglobal platforms and reducing the risks related to implementation in each \nMember State. \n\np. 4 \n \nForeword \n \nThe mission’s objective was to explore a general framework for the regulation of the social networks, \nstarting from the fight against online hatred and relying on the voluntary cooperation, outside any legal \nframework, of Facebook1 (see the appended mission letter).  \nThe purpose of this experiment is to explore how a new system to regulate social networks could be \nestablished to complement existing instruments and better achieve public policy objectives in terms of the \nreconciliation of public freedoms and the safeguarding of public order on social networks. Although the \nexchanges with Facebook thus focused on hate content, the mission’s conclusions may be applied to all \nthe issues raised by the publication of content on social networks. \nThis interministerial mission team2 comprises seven high-level experts and three permanent reporters \nfrom a range of ministries – Culture, Interior, Justice, Economy, Prime Ministerial services - DILCRAH3 \n(Interministerial Delegation to Combat Racism, Antisemitism and Anti-LGBT Hate), DINSIC4 \n(Interministerial Delegation of Digital and Information and Communication Systems) and independent \nadministrative authorities - ARCEP5 (electronic communications and postal authority) and CSA6 \n(audiovisual regulatory authority).  \nThe mission worked with Facebook throughout January and February. Over the course of several working \ndays with the mission in Paris, Dublin (location of its European headquarters) and Barcelona (location of \none of the moderation centres), Facebook’s representatives presented its policy for moderating hateful \ncontent, its organisation, and the resources it devotes to this as well as its internal procedures. Meetings \nwere held to examine specific topics in depth, including the use of algorithms in the moderation system to \ndetect hateful content and the basic principles of algorithms that order content for Facebook users. \n \nAlthough the mission received a very open welcome from Facebook, it did not have access to particularly \ndetailed, let alone truly confidential information. This was due to the speed of the work, the lack of a \nformal legal framework and the limits of Facebook’s transparency policy. The mission is nevertheless \nconvinced that this limitation did not affect its results, as its goal was not to evaluate the relevance of \nFacebook’s mechanisms, but to imagine “rules of the game”7, which could be adopted by the legislator to \ncreate a long-term regulatory framework for global actors operating abroad, such as Facebook.  \nIn this respect, the report does not detail Facebook's mechanisms for moderating the fight against the \ndissemination of hateful content online. Nevertheless, the reader can refer to documents published by \nFacebook to better understand the mechanisms for moderating the social network and in particular the \ncommunity standards, the report on the transparency of content management on its platform, and the \n                                                            \n1Following an agreement between the company’s Chairman, Mark Zuckerberg, and the President of the Republic announced at \nthe Internet Governance Forum in November 2018. \n2The composition of the mission is appended to this report. \n3 Délégation interministérielle à la lutte contre le racisme, l’antisémitisme et la haine anti-LGBT is France’s Inter-ministerial \ndelegation for the fight against racism, antisemitism and anti-LGBT hatred. \n4 The Direction interministérielle du numérique et du système d'information et de communication de l’État is the Interministerial \nDirectorate for Digital Technology and the Government Information and Communications System. \n5 The Autorité de régulation des communications électroniques et des postes regulates France’s telecommunications. \n6 The Conseil supérieur de l’audiovisuel regulates France’s electronic media. \n7The mission is convinced in this respect that the regulators’ first power should be its right to demand the communication of any \ninformation necessary for the accomplishment of its mission in a legally enforceable framework. \n\n \n \n \n \np. 5 \n\"hard questions blog” on which Facebook regularly publishes reflections on the subject of moderation (in \nEnglish). \nThe mission also presented its approach to associations fighting hate speech, at a seminar organised by \nCNNum (National Digital Council) on 14 February and 15 February 2019.  \nThe mission finally completed its work with a study trip to Berlin to better understand the experience of \nthe German NetzDG law, a mission in London and a series of meetings with public operators – Inria, \nPlatform Pharos; Centre de lutte contre les criminalités numérique, Secrétariat général aux affaires \neuropéennes, Conseil national du numérique, Direction générale des entreprises, Direction générale du \nTrésor), and private entities and NGOs (Reporters sans frontières, le CERRE, la Quadrature du net, \nWebedia, Netino, Snap, Google, and Twitter). \nThis report formulates proposals which, if adopted, need to be fleshed out. This report (1) identifies some \nkey features of social networking services, (2) analyses some public policy approaches to those services \nand (3) recommends creating a new regulatory system based on five pillars and (4) presents a focus on the \nconcept of the transparency of algorithms and its implementation. \nDue to scheduling constraints, several topics were not examined. In particular, the mission did not \nconduct a study of the competitive impact of the proposed regulatory scheme on other social network \nservice offerings. However, the regulatory system should be careful not to create an insurmountable entry \nbarrier for mid-sized market players or new entrants, and, consequently, to unduly favouring the \nconsolidation of the hegemonic actors by enacting regulatory barriers. \nFurthermore, the mission focused its study on public content, but it is clear that hateful content is also \npresent in private or closed groups on social networks, and that there is currently a trend for increasing \ndissemination of content within these limited groups and on messaging services. It is more complex to \nintervene on these environments where exchanges can be covered by the secrecy of private \ncorrespondence and especially encrypted \"from end to end\", rendering illusory any moderation by the \nplatform itself since the content exchanged between the users has no visibility. \nFinally, the report does not deal with intervention methods for “non-cooperative” social networks which \ndo not correspond to a traditional economic rationale, whether they are militant extremists (4chan, 8chan, \netc.) or controlled directly or indirectly by a sovereign state pursuing political objectives. \n \n \n\n\n \n \n \n \np. 7 \nI –  Social networking services \n \n \nBy enabling everyone to publish content and share it with other users, social networks are revolutionising \nthe media industry and communications by offering individuals and civil society a direct means of \nexpression. Nevertheless, the possibilities offered by social networking services give rise to unacceptable \nabuses by isolated individuals or organised groups, to which the operators are not providing a sufficient \nresponse or are even contributing to via their content ordering systems.  \n \n1.1  \nAlthough the purpose of all social networking services is to share and disseminate content \nto the public online, they are nevertheless very diverse  \n \nSocial networking services are defined by the ability to disseminate content produced by their users to all \nor some of the other users on that network. \n \nSocial networking services are provided by different types of operators, differentiated by their legal status, \ntheir type, their target, their economic model, the type of content published8 and their distribution \nmethods. This falls into two categories: \n- \nSocial networking services offered on an ancillary basis: Thematic or general discussion forums \non websites (e.g. jeuxvideo.com and comment spaces on media websites such as lemonde.fr or \nlefigaro.fr) constitute a basic form of a social network: content organisation is rudimentary \n(mainly by chronological order) and, when monetised by advertising, this is usually not combined \nwith user content, but is adjacent; \n- \nSocial networking services offered as the main focus: Social networking platforms, which may be \ngeneral, like Facebook or Twitter, or structured around a particular content type or format, like \nYouTube (videos), Pinterest (photos), TikTok (short videos) or Snapchat (short videos and \nphotos). \n \nAll of these services offer some or all of the following content and features: user-generated content \n(UGC) as the main content, promotional content9, content from professional publishers, content \naccessible to everyone and/or content which is restricted to a select group of users, individual accounts \nwith or without a screen name, or discussion areas attached to a community or an event.  \nThe content ordering system on a social networking service may be personalised (i.e. specific to each user) \nand be more or less sophisticated depending on the volume of published content. \nPlatforms increasingly offer a private messaging service with social networking services: Direct message \nfor Twitter, Messages for YouTube, Messenger (and WhatsApp) for Facebook. In some cases, as with \nSnapchat for example, content dissemination to a closed group may be the default option, but the user \nmay make the content available to the entire network at any time.  \nMonetisation methods vary widely from one service to the next, from user-independent advertisements \nand shared content, to advertisements targeted according to the user’s favourite content or targeted \naccording to the user viewing the content. Revenues may be shared with the content publisher, as is the \ncase for YouTube and Snapchat. \n                                                            \n8 The content may be a text, a hyperlink, an image, a sound, a video (sometimes in real time), a computer programme and/or any \ncombination of these six elements. \n9 This may be traditional advertising, a product placement, sponsored content, etc.  \n\np. 8 \nBeyond the distinction by type of operators and functionality, the size of the social network is a central \ncriterion to take into account: from 2 billion users for Facebook, with a worldwide presence, to a few \nthousand users on some discussion forums.  \n \nFinally, models and uses are not static and in fact evolve very quickly. Services generate new uses while, \nconversely, user behaviour is constantly inspiring operators to adapt their services.  \n \nAn attempt at a legal definition of a social networking service was first introduced in the bill proposed by \nFrench MP Laetitia Avia, designed to combat online hatred. The definition of operators of online \nplatforms set out in Article L.111-7 of the French Consumer Code isolates operators of online platforms \n“offering an online communication service to the public based on connecting several parties in order to share public content”.10 \nAt European level, the Audiovisual Media Services Directive incidentally introduces the concept of “social \nmedia services”. Its Article 1 defines “audiovisual media services” and it states, in recitals (4) and (5), that \nsocial audiovisual media services are those whose content is created by users.11  \n \nA social network may be defined as an online service allowing its users to publish content of their \nchoice and thereby make them accessible to all or some of the other users of that service.  \n \n \n \n1.2 Social networks are revolutionising the media industry and communications by offering \nindividuals and civil society a direct means of expression. In that sense, they represent a great \nstep forward for freedom of expression  \n \nWith a capacity to host and distribute mass content for a very low marginal cost, social networks are a new \nform of media enabling direct expression, without pre-selection of authors or content, or any journalistic intermediation. A \nsocial network allows the exchange of content that it has neither created nor pre-selected, subject to \ncompliance with rules of publication issued by the social network (see below). This lack of creation or \nselection, which distinguishes social networks from traditional news media, allows everyone to express \nthemselves, to publicise and disseminate their opinions or content of their choice and to access new sources of \ninformation. The ability of an individual, an association or a private or public operator to express itself \npublicly is no longer dependent on the editorial choices of traditional media. \nThese networks are creating new forms of social relations, transcending geographical limitations (and even \nlinguistic limitations as a result of translation tools) and subverting both historical social structures and the \nprimacy of the territorial organisation of states and our societies. New “digital” associations, intangible yet \nvery real communities, have sprung up to share information or areas of interest or to unite around a \ncommon cause.  \nThe opportunities offered by these new communication vehicles are reflected in user behaviour. Social \nnetworks are now vital tools for accessing and disseminating information. One-third of French people and \n                                                            \n10 It is marked by the definition set by German legislators: the German NetzDG, adopted in 2017, defines the social networks of \nfor-profit internet platforms, which is intended to allow users to share any content with other users or to make that content \naccessible to the public. Platforms managing editorial or journalistic content are excluded. Law no. 2018-1202 of 22 December \n2018, relating to combating the manipulation of information, targets all operators of online platforms within the meaning of the \nFrench Consumer Code.  \n11When video sharing is an “essential feature”, those services fall into the category of “video sharing platforms”, distinct from that \nof “audiovisual media services” and subject to simplified regulations mainly intended to protect young users and tackle the \ndissemination of hate content. \n\n \n \n \n \np. 9 \nhalf of 18 to 24-year-olds obtain their information from social networks, while video-sharing platforms \nrepresent half of all news videos watched on the internet.12  \n \n1.3 Social networking services define content ordering and therefore exert a form of de facto \nrather than legal editorialization, which is generally unobservable and non-transparent  \n \nAll of the content published on a social network cannot be presented to users without ordering. The \nvolume of content published necessarily requires the platform to define an order to display and to carry \nout a selection, without prejudice to the user's ability to search for specific content if they so choose. The \ncontent which the user will actually view will depend firstly on the layout of his or her interface and the use \nof algorithms to prioritise and personalise presentation of the various content. Unlike traditional media, \nthe ordering of content on social network services is usually personalised (except in forums) and everyone \nsees the result of the personalisation when accessing the service. However, the overall effects of this \nordering on all users are not observable.  \nFurthermore, the operators providing social network services do not always reveal the precise criteria used \nto define the presentation of content. These ordering criteria may be numerous, and their weighting varies \naccording to the purpose of the service (supposed interest of the content, identity of the author, whether \nthey are paid-for, user’s preferences and behaviour, etc.). More generally, the ordering function gives \noperators of social network services the capacity to accelerate or, on the contrary, slow down the \ndissemination of certain content. \nThe existence of this function of ordering content, constituting a form of de facto editorialization, cannot \nquestion the legal status of the operators or lead to legal requalification of hosting providers as publishers, \nsince the majority of social network services do not carry out any selection prior to the publication of \ncontent. \n \nThe existence of this information organisation system plays a key role in the dissemination of \ninformation and in social networks’ ability to prevent or increase damage to social cohesion. \n \n1.4 The freedoms of communication and public expression offered by social network operators \nlead to unacceptable abuses by isolated individuals and organised groups to which the social \nnetworks are not providing an adequate response \n \nWhether paid-for, free or paying, the majority of content published on social networks does not pose any \ndifficulty13. As a result of this capacity for large-scale communication and expression, however, combined \nwith a feeling of relative anonymity and impunity, social networks are also forums for the exchange of \nunacceptable content and behaviour (content inciting hatred, terrorist content, child pornography, online \nharassment and identity theft) which can have a significant impact on social cohesion and harmony \n(spreading of false information and unfounded rumours, attempts to fraudulently manipulate public \nopinion by individuals or groups with political or financial objectives).  \nMost operators have implemented terms of use which indicate the categories of content which are \naccepted on the service as well as moderation mechanisms when those rules are not respected by users. \nGiven the volume of content published and the statistical approach taken by algorithmic tools, however, \nsocial network operators are currently unable to prevent all risk of their services being abused. In fact, the \n                                                            \n12 Reuters Institute, Digital News Report 2018.  \n13 Out of 10,000 Facebook content views, for example, between 23 and 27 apparently contain scenes of explicit violence \n(Facebook’s transparency report, figures for Q3 2018).  \n\np. 10 \nefforts deployed are still largely perfectible, especially by those with a large audience. In addition, little \ninformation is made public on how terms of use are defined and implemented or how the moderation \nsystem works. \nThe fight against the dissemination via social network services of harmful content to users and social \ncohesion involves looking at how rules are defined, moderation of content already posted and, potentially, \ntheir ordering system, particularly if it involves the personalisation of content.  \n \nEven if the abuses are committed by users, social networks’ role in the presentation and selective \npromotion of content, the inadequacy of their moderation systems and the lack of transparency of \ntheir platforms’ operation justify intervention by the public authorities, notwithstanding the \nefforts made by certain operators. \nThe development of public policies designed to prevent abuses and misuse of social networks \ntherefore appears necessary but should be subject to particular precautions in several respects.  \nFirstly, it will be necessary to take into account the diversity of operators providing these types of \nservices and, at least initially, to concentrate on those with the most influence over our societies.  \nSecondly, any state intervention must be strictly necessary, proportionate and transparent \nwhenever it affects public freedoms that are as important as the freedom of expression and \nfreedom of communication.  \n \n \n\n \n \n \n \np. 11 \nII –  Promoting a new public policy approach  \n \nThe inadequacy and lack of credibility of the self-regulatory approach adopted by the largest platforms \njustify public intervention to make them more responsible. That intervention must be based on a balance \nto be defined between the punishment of authors of harmful content and pragmatic and flexible ex ante \nregulation of operators providing social networking services, within a revised European framework.  \n \n \n2.1 \nPlatforms that are using self-regulation with limited results \n  \nThe work carried out with Facebook, supplemented by discussions with other operators, show that the \nplatforms are striving to develop a self-regulation approach. \nIn the case of Facebook, the mission found that the system has self-regulatory mechanisms endowed with \nincreasing dedicated resources:  \n \nrecent transparency on the detailed content of “community standards”;  \n \nincrease of human resources and development of mass processing algorithms dedicated to the \nmoderation system; \n \ncurrent development of “distributed” moderation tools available to users; \n \ninternal organisation of the function of moderation, publication of transparency reports; \n \nattention to the establishment of open governance structures extending beyond platform \nrepresentatives, in particular a supervisory board made up of independent experts, responsible for \nreviewing moderation decisions. \n \nAs for YouTube, the self-regulatory approach of Google’s video-sharing platform includes tools to \neducate users of the platform in prohibited behaviour or if they are the victims of the aggressive \nbehaviour of other users. For example, the platform has employed influencers to try to change user \nbehaviour, especially among the youngest of them. However, the effectiveness of what seem a priori \ncommendable initiatives remains to be assessed.  \nIn addition, the mission was able to observe that the moderation system not only involves the removal of \ncontent considered toxic, but that there is a range of possible responses, depending on the type of content \nand the degree of potential damage it could cause (quarantining, hiding with a prevention message, de-\nreferencing, warning, etc.). \n \nThe speed of deployment and progress made during the last 12 months by an operator such as \nFacebook show the benefits of capitalising on this self-regulatory approach already being used \nby the platforms, by expanding and legitimising it.  \n \nThe self-regulatory capacity observed at these operators providing a social network makes it \npossible to position them as key elements in the solution to social cohesion issues raised by the \npresence of certain content on these platforms.  \n \nThis solution cannot be reduced to simply removing obviously illicit content, but must be \nenhanced in order to avoid harm (prevention) and respond in all possible situations based on \ntheir severity and the risk to users: quarantine, deceleration, demonetisation, reminder of the \ncommunity rules, targeted education, etc. \n \n \nNevertheless, even anticipating the full effect of operators’ stated ambitions, the mission found that it \nwould not offer a sufficient response to public policy concerns: \n\np. 12 \n The extreme asymmetry of information between social network operators, on the one hand, \nand public authorities and civil society, on the other, considerably undermines the \ncredibility of a self-regulation approach. \n \nNeither the public authorities nor civil society know how much credence to give to operators’ \nstatements. They have access to practically the same level of information as a user. None of the \ninformation made public by the platforms concerning their self-regulatory actions can be \ncorroborated by objective facts. This limitation is inner to the functioning of the main social \nnetwork services, due to the personalisation of the content provided. Creating an account on \nthese platforms allows users to see only a tiny fraction of them. Only the platform itself is able to \nmeasure impacts at a global scale.  \n \nThis lack of credibility is heightened by the enormous volume of content and number of users of \nthe platforms, necessarily requiring processing by algorithms based on a statistical approach. \nBeing unable to prove the existence of a systemic failure by the platform, the public authorities \nand representatives of civil society are reduced to highlighting individual examples of \nunmoderated or poorly moderated content. Yet these isolated failures are insufficient to prove a \npotential systemic failure. \n \n \nThe persistent dissatisfaction of the public authorities can be explained in particular by \ntheir inability to assess the measurable reality and value of the self-regulation carried out \nby these operators, due to a lack of information validated by a trusted third party. \n \n \n Self-regulation remains too “inward-looking” \n \nNo doubt because of its lack of maturity, self-regulation remains unconvincing because social \nnetwork operators hold all the cards: they draw up their terms of use, decide to what extent to be \nbound by them, modify them as necessary without any public formalities, interpret them without \nthe possibility of appeal and report on their implementation in the form and frequency they \nconsider appropriate.  \n \nDue to their recent, competing and disparate nature, social network services have each developed \ntheir own model of self-regulation. The minimum level of credibility normally provided by a \nsectoral approach, which allows for “peer review” – e.g. the approach taken by the ARPP (French \nAdvertising Regulatory Authority) – is absent here and does not seem to be contemplated. \n \n Self-regulation is agile but is not subject to any form of supervision \n \nSocial network platforms are agile. They have developed with an entrepreneurial spirit which, \nsometimes deliberately, disregards certain regulatory constraints in order to preserve their ability \nto innovate. They constantly test the efficiency and relevance of their user interface, their \nalgorithms and the organisation of their moderation function, particularly using A/B testing14. \nVoluntarily giving up, even partially, what has been their main strength therefore remains a major \nchallenge for their management teams.  \n \nToday, it may well be argued that the major platforms are developing a self-regulatory approach \nnot in order to assimilate and fully address general public policy objectives, but rather to contain \nany risk of coercive intervention by the public authorities and pressure from civil society, in order \nto avoid damage to their reputation. In this context, all of the initiatives taken, however relevant, \nlack credibility and are difficult to assess. \n                                                            \n14 Technique involving testing a feature, editorial, graphical interface or new algorithm on two different groups of users to assess \nits effects.  \n\n \n \n \n \np. 13 \n \nToday, as far as the mission is aware, no social network has adopted truly enforceable rules in \nterms of providing users with information about terms of use, processes to amend them or \nmechanisms to involve civil society or the public authorities in their development, even in an \nadvisory capacity. The credibility of the self-regulatory approach clearly suffers as a result.  \n \nAn approach that puts the social networks at the heart of the regulatory model seems quite \nrelevant. In this model, the social network platform incorporates public interest objectives, \nmodifies its organisation, adapts itself to this “social” objective and acts either upstream at the \ndesign stage, to prevent difficulties, abuses and other misuse of its service, or downstream, to \naddress unacceptable behaviour by its users.  \n \nTo borrow the GDPR term “Privacy by design” relating to personal data, we could speak of \n“Accountability by design” for the processing of content by the social networks. \n \n2.2 \nDeveloping a public policy to make the platforms more accountable  \nThe aim of the public intervention model is therefore not to regulate activity, i.e. impose functional or \ntechnical constraints on the services provided, but to make the social network operators more accountable \nby a legally binding obligation to come up with resources and to be accountable. Such a model, insofar as \nit minimises public interference in the functioning of a media industry whose core purpose is to serve as a \nmedium for individual expression, would also have the virtue of minimising criticism concerning the risk \nof the manipulation of information by the public authorities. This criticism is inherent to the industry’s \npurpose. It should not deter public intervention, but it does call for special precautions. \nMore direct regulatory interventions, such as those in the energy, transport, telecommunications, the \ntraditional audiovisual media and online gambling industries, would also seem to be less appropriate, since \nthey involve activities clearly attributable to a given national territory and therefore to the jurisdiction of a \nsingle regulator. However, the social networks often transcend geographical national borders. A discussion \nof content written in French inciting hatred of refugees, published by a user located outside the European \nUnion, may be the subject of comments, also potentially hateful, by a set of other French-speaking users \nlocated anywhere in the world. \nAn intervention method using co-regulatory mechanisms that imposes the internal \nassimilation of public interest objectives, without defining the methods, would make it \npossible to limit the impact on social network services.  \nThis new method of public intervention, focused on creating a duty to defend the integrity of \nthe social network and its members, on the one hand, and on improving the credibility of self-\nregulation, on the other, would not undermine the founding principles of social networks, in \nother words their unique, ubiquitous and agile nature. \n \n2.3 \nA public policy dynamic that must find a balance between a punitive approach and \nmaking social networks increasingly accountable through preventive regulation \nIn several European countries, the initial public policy response to issues identified on social networks has \nbeen to implement or strengthen punitive sanctions targeting the authors of content deemed unlawful as \nwell as the platforms, which, because they display and host the content, appear to bear the same liability as \nthe author, or at least to be “accomplices”. \nThe punitive policy is necessary in that it expresses the rules adopted by society in a clear and visible way. \nIt is also required on a purely political level in situations of manifest disruption of public order. The \npunitive policy is effective only if it is comprehensible and enforced, so as to avoid any feeling of impunity \nfor the authors of unacceptable content. \n\np. 14 \nThe punitive policy, in particular because it opens up the possibility of recognition of harm and its \ncompensation, is an essential tool, but it cannot achieve all public policy objectives. It is limited by the fact \nthat it necessarily intervenes ex post, to sanction unlawful behaviour recognised as such by a court. The \npowers devolved to criminal and civil authorities and the legislative timetable currently make it impossible \nto anticipate changes in social networks and the disruption they can cause. \nUnlike traditional media, social networks do not select each item of content published on the service. This \nis a defining characteristic of such services. Punitive measures against them therefore raise several \ndifficulties. The social network finds itself in the position of a censor, ex post, of all users’ posts on its \nnetwork, essentially after these are signalled (using the platform’s interface) or notified (LCEN’s specific \narrangements) by users or the public authorities. By imposing an absolute standard of conformity that \ndoes not take into account the volume of the published content, the audience or the statistical nature of \nthe processing, punitive measures risk encouraging over-moderation and thereby infringing freedom of \nexpression, which is constitutionally and conventionally protected. \nMoreover, the punitive approach requires the platforms to judge the manifest lawfulness of a content \nthemselves. They consider that this lawfulness is particularly difficult to assess from the triple perspective \nof the legislative intention, prosecution practices – which are by definition more selective depending on \nthe chances of prosecution – and case law of national and international courts, which is based on \nbalancing freedom of expression against public order imperatives. To the best of our knowledge, the \nestablishment of “guidelines on manifestly hateful content” by an administrative authority, even an \nindependent one, does not seem to be a very satisfactory solution. \nLastly, the scope of content that is not “manifestly unlawful” (grey zone) varies according to geography \nand does not easily lend itself to European or international harmonisation, particularly when it comes to \ncontent that could be qualified as inciting hatred, because of its historical, cultural and legislative \nassociations specific to each state. Particularly since punitive measures are often implemented or \nstrengthened after a crisis arousing strong public opinions and calling for, and authorising, a strong \npolitical response. Except in exceptional cases, these crisis situations are local and do not cross borders (or \nto a minimal extent). The conditions for supranational harmonisation of punitive responses are therefore \nvery difficult to meet. \nPunitive measures should be supplemented by the adoption of a second public policy \ncomponent designed to make platforms more accountable by establishing an obligation of \ntransparency and to defend the integrity of the social network and its members, by creating \ntargeted and comprehensible incentives. \n \n2.4 \nEuropean cooperation to be reviewed \nAt the current stage of European integration, social cohesion is primarily established at the level of \nMember States. Although services are global, the damage resulting from their existence occurs at a \nnational level. In Europe, due to the different languages in use, communities on social networks are \nformed on the basis of linguistic affiliation and most frequently on a national or sub-national basis. \n \nThe accountability of social network operators should be organised at the level of Member \nStates, which are more directly affected by abuses, rather than at the level of the European \nUnion itself, which remains removed from crises and their consequences in terms of public \norder and social harmony. \n \nAs a result of the coherent legal space it offers, the European Union nevertheless is in a unique position to \nallow Member States to act coherently in respect of global and geographically ubiquitous operators. The \n\n \n \n \n \np. 15 \nEuropean Union offers the ability to bring the combined weight of Member States to bear against the \npower of the large social networking platforms when those states adopt the same standard of regulation.  \nThe European Union is also in a strong position to reduce the risks of failure or excessive regulation by \npublic policies, by reducing political risk at the level of each individual Member State. This capacity has \nparticularly been demonstrated in the telecommunications sector, where the transformation of public \nmonopolies into a competitive industry has benefited significantly from the European capacity to \nmoderate occasionally excessive regulatory decisions and to overcome national inertia.  \nNevertheless, the opportunities to take advantage of European construction require a new ambition:  \n \nThe dialogue between the operators, the Member States and the European Commission on the \nmonitoring of the self-regulation of illegal content seems to be bearing fruit in light of the results \nof the fourth evaluation of the EU Code of Conduct on combating illegal online hate speech, \npublished last February15. Nevertheless, there is undoubtedly room for progress: this initiative \nsuffers from the great distance between the European level at which implementation occurs and \nthe location of the damage. Moreover, although the commitments provided for in the framework \nof this approach are relevant, the voluntary nature of the commitments, without any penalty \nmechanisms, is revealing its limitations (see above); \n \n \nthe legal regulatory framework was built around the principle of the country of origin, giving \nexclusive regulatory responsibility to the Member State in which the service is established. The \nability of each Member State, apart from the one hosting the service, to tackle any mistakes by a \nglobal player is therefore drastically reduced by this European cooperation, therefore increasing \nthe political risk in the destination country (in which the damage is produced). Yet the authorities \nof the country where a breach by the service was identified are those best placed to establish and \ncorrect that breach, especially when it involves assessing an abuse of freedom of expression (e.g. \nthe publication of hate content) which must be assessed in light of the social, political, cultural \nand historical context of the state affected. Moreover, the political risk between Member States is \nalso simultaneously increased since a single Member State receives the exclusive benefit from the \nplatform's establishment on its territory, reducing its incentive to intervene in the event of \nbreaches by a platform while other Member States suffer the potential damage and remain \npowerless to act; \n \n \nThis structure, based on the jurisdiction of the country of origin, is reflected in the Audiovisual \nMedia Services Directive and in the version of the draft regulation to combat the dissemination of \nterrorist content adopted by the Council of Member States last December. \n \nThe establishment of a European regulation based on the principle of jurisdiction of the \ninstallation country would strip the Member States of their ability to take action against the \nlarge social network platforms present on their territories. \n \nThe establishment of national regulations by each Member State would produce a high risk of \nincompatibilities between those regulations due to the unique and ubiquitous nature of social \nnetworking services, which would then become subject to contradictory and therefore \nineffective rulings. These national systems would be exposed to the risk of non-compliance \nwith treaties.  \n \nIn contrast, cooperation based on a European regulation and the principle of the “destination country” \ncould reinforce each Member State's capacity to cope with the difficulties generated by global players such \nas the largest social networks, while reducing the political risk for those players: \n                                                            \n15 http://europa.eu/rapid/press-release_IP-19-805_fr.html \n\np. 16 \n \nBy organising the platform's accountability according to the destination country, it creates an \nincentive for each social network to prevent abuses in each region. This restores geographical \ncoherence between the location of the regulatory function and the location of the damage; \n \nA common framework, laying down uniform obligations, defined at European level via a directly \napplicable regulation, would guarantee the consistency and uniformity of the legal standard across \nall regions. This is essential in respect of global players enjoying practical ubiquity. Each Member \nState becomes responsible for implementing a common rule within its territory. This enables \ncoordinated action by Member States and their regulatory authorities. It also allows the \nestablishment of mechanisms to mitigate the political risk for each Member State at a European \nlevel using various proven mechanisms, including peer review, establishment of a board of \nregulators, direct supervision by the European Commission and judicial review by the ECJ. \n \nThese structures have already been tested and implemented in respect of the regulation on net neutrality \nin the European Union. \nIn the absence of such a mechanism, and in view of the issues at stake, Member States risk unilaterally to \nlaunch legislative initiatives with disparate scopes and obligations, as demonstrated by the German law, to \nthe detriment of the digital single market approach and to the benefit of existing players which would be \nthe only ones able to support the economic burden of a series of national regulations (thereby \nencouraging a “winner takes all” outcome). \nThe challenge of setting up a French regulatory framework for social networks must be viewed \nin light of its ability, under the new EU presidency, to serve a proposal to: \n \nreverse the current European trend, move away from the logic of the installation \ncountry, which weakens the sovereignty of Member States and their capacity to tackle \nglobalisation, and switch to a destination country approach in order to strengthen the \nMember States; \n \nreduce political risk; \n \nand increase the legitimacy of European integration.  \n  \nUnlike the punitive approach, the establishment of an ex-ante regulatory framework, adopting \na compliance approach that is focused on creating strong incentives to make the social \nnetwork and its members accountable as well as strengthening the credibility of self-\nregulation, offers a unique opportunity to propose a change in Europe’s orientation.  \n \n2.5 \nA regulatory policy based on a compliance approach to be applied and designed \nwith pragmatism and agility \n \nIn the financial sector, governments have attempted to promote the credible and long-term commitment \nby financial institutions to actively contribute to achieving the public interest objectives of combating \nmoney laundering, drug trafficking and the financing of terrorism. Banking supervisory authorities have \ntherefore devoted their efforts to imposing and monitoring obligations of means, i.e. compliance with \ncertain preventive rules, rather than punishing failures when the risks being combated materialise (without \nprejudice to criminal proceedings in that case). Therefore, the banking supervisory authorities do not \nintervene when it is found that a financial institution has been the channel for channelling funds used for \nunlawful purposes, but when it finds that a financial institution is not implementing a prescribed \nprevention measure, regardless of whether or not the financial institution is implicated in unlawful \nbehaviour. This intervention approach is designed to create targeted incentives for platforms to participate \nin achieving a public interest objective without having a direct normative action on the service offered.  \nApplied to social networking services, this type of intervention involves identifying the few generic \nobligations likely to create the right incentives, particularly by increasing the effectiveness of political \n\n \n \n \n \np. 17 \ndialogue with Member States’ political institutions and their civil societies. This implies imposing strong \nobligations on the transparency of key systems unobservable from the outside, i.e. the moderation system \n(and procedures for developing and updating the terms of use that underlies it), as well as the use of \nalgorithms for targeting and personalising the content presented.  \nThis approach must be implemented progressively and pragmatically according to the size of the \noperators and their services: \n \nOnly services with the most influence due to their size – and therefore the most dangerous in \nterms of their potential impact in the event of abusive use – should be subject to these obligations \nand to ex ante compliance checks by the regulator. The regulator should focus mainly on active \nsupervision of systemic actors. \n \n \nMid-size services should be allowed an initial presumption of compliance and receive support \nfrom the regulator, which could encourage their accountability commitment by issuing \nrecommendations and implementing measures to increase pooling of common assets (e.g. \ndatabase identifying unlawful content, access to annotated data sets for machine learning by \nmoderation algorithms) with the largest platforms, in an open, transparent approach resulting in \nlowering barriers to entry. Nevertheless, if the regulator finds that a mid-size service is in clear \nbreach of these obligations of means and that this results in excessive manifestation of the \nharmful effects being combated by the public policy, the regulator must then be able to ask the \noperator providing that service to take appropriate measures and, if these measures are not \nimplemented by the platform, also be able, in a reasoned decision, to impose an enforceable ex \nante compliance procedure on it, similar to that imposed on the most influential services. \n \n \nFinally, concerning the smallest platforms, the regulator must be able to act only through dialogue \nand issuing recommendations, but without coercive action, without undermining its capacity to \ncall the procureur’s attention to any act that might be subject to criminal procedures.  \n \nMechanisms enforcing penal and civil accountability, particularly under the LCEN (French law to \npromote confidence in the digital economy), nevertheless remain applicable to all social networking \nservices, regardless of their size. These are not subject to the regulator’s action but rather to judicial bodies \nand common law procedures. \n \nThe legislative framework should allow gradual implementation of these mechanisms and \nrecognise the new regulatory system, as well as a capacity to define and gradually refine the \nobligations imposed, taking an agile approach in order to adapt quickly to changing social \nnetworks. In other words, the law establishing this regulatory framework must define the \nnature of the obligations, without seeking to define detailed procedures. Otherwise, the \nregulatory framework could easily be bypassed by “overly” agile operators. \n \n \n\n\n \n \n \n \np. 19 \nIII -  Organisation of the regulatory function in France  \n \nwithin a framework defined at the European level \n \nIt appears necessary to supplement the punitive measures against authors publishing unlawful content or \nseeking to manipulate social networks with a proactive dialogue approach, based on strengthening the \npolitical dialogue between the public authorities and the actors concerned. Creating the conditions for \nconstructive and regular dialogue with social network platforms should enable them to switch to the \nproposed solutions by encouraging them to adopt a responsible approach to their users and society and to \nprevent abusive use of their services. \nThis regulatory policy could be based on the following five pillars: \n \n \nFirst pillar  \nA public regulatory policy with broad objectives \nguaranteeing individual freedoms and entrepreneurial freedom  \nTo unite political energies both at national and European level and to bring together political institutions \nand civil society, the objectives of the regulatory system must be to defend the exercise of all rights and \nfreedoms on social media platforms: \no Individuals’ freedom of expression and communication, with individuals therefore being entitled \nto understand how the platform respects that freedom; \no Individual freedom of users to be protected in their physical and moral integrity, including on \nsocial networks in the digital space; \no Social networks’ entrepreneurial freedom, including the right to define and apply terms of use, to \nexercise an unrestricted information ordering system and to innovate (especially for smaller \noperators). \n \nThe objectives could also include secondary objectives of: \no pluralism of social network services and therefore a public policy position ensuring that new \nservices are supported and that no entry barriers are created; \no social cohesion, by encouraging the social networks to develop “positive” uses of their services, \ni.e. that strengthen social relations. \n \n\np. 20 \nSecond pillar  \nA prescriptive regulation \nfocusing on the accountability of the social networks, \nimplemented by an independent administrative authority \n \no A regulation with coercive action limited to the sole organising platforms at the level of each \nMember State, with two thresholds:  \n \n \nThe regulation would automatically apply for services for which the number of monthly users \nrises beyond a certain percentage of the population of the Member State (between 10% and \n20%).  \n \nThe regulatory system would also be applied only following a reasoned decision by the \nregulator in the event of an identified and persistent breach for services with a monthly \nnumber of users lying between 0% and 5% of the population of the Member State. It should \nbe noted that the lower this second application threshold, the more stringent and demanding \nthe impact test must be in order to comply with a general principle of proportionality.16 \n \nThe regulation is not applicable below these thresholds, but the common law provisions of \nthe LCEN remain in force, allowing action for civil and criminal liability of the operators in \ncase of breaches.  \n  \no Transparency obligations that concern the key functions of the social networks17: \n \n \nThe function of “ordering content”: that is to say, the methods of presentation, \nprioritisation and targeting of the content published by the users, including when they are \npromoted by the platform or by a third party in return for remuneration; \n \nThe system for implementing the terms of use and moderating content, including the \nmethods for the elaboration of these community rules, the procedures, the human and \ntechnological resources implemented to ensure compliance with these Terms and to fight \nagainst illegal content. This system must be able to be audited by the regulator and/or by an \nindependent auditor of the platform. Transparency can be seen in, for example:  \n \nThe obligation to notify the platform's decision to the author of moderated content \n(except legitimate exceptions, e.g. if required by the public prosecutor) and the \nperson who flagged the content (where applicable); independent and extra-judicial \nmechanism for reviewing the platform's decision (without prejudice to a judicial \nremedy); \n \nUse of automated processing tools: what tools are used, for what types of content, \nwith which human supervision? How is their effectiveness and accuracy assessed? \n \nProcedures for cooperating with “trusted flaggers”: list, selection procedures, \n“privileges” attached to that status, statistical data on the number of reports \nexamined, the number of contents detected proactively, the follow-up given \n(removal, maintenance, etc.), the appeals processed, etc.  \n                                                            \n16 This second threshold could be replaced by a “malfeasance” criterion in order to give the regulator the capacity to tackle any \nsocial networks raising issues. Nevertheless, the question arises of the appropriate level of the regulator’s resources and the \nnecessity of creating a criterion for abandonment by the regulator in respect of small operators that no longer raise issues. \n17 The details of the transparency obligations set out below are given for illustrative purposes. They do not necessarily need to be \nincluded exhaustively in the text of the law defining the regulatory system and many may be subject to the regulatory discretion of \nthe government or the regulator. \n\n \n \n \n \np. 21 \n \nStatistics concerning moderation efficiency: decision times (whatever the decision \nmay be), false positive/negative rates, virality/audience of content contrary to \ncommunity standards before it was withdrawn (see concept of prevalence), etc.  \n \no Obligation to defend the integrity of the social network and its members \n \nBy this obligation, which is close to the Anglo-American “duty of care”, the social networks are \nresponsible for protecting their integrity and that of their members, i.e. to protect users from abuse by \nother members and third-party attempts to manipulate the platform.  \nThe obligation of means would allow intervention by the public authorities if it appeared that platforms’ \napproach, currently voluntary, to ensuring that their users can have confidence, through the creation of \n“trust and safety” systems or the moderation system, lack resources. \n\np. 22 \nThird pillar \nBroad, informed political dialogue conducted transparently \nbetween the government, the regulator, the actors and civil society \no Using its regulatory power, the government sets the thresholds for triggering obligations and defines \nthe terms of the transparency obligations applicable to the functions of ordering content and \nimplementation of terms of use, as well as to the obligation to defend the integrity of the social \nnetwork and its members. \n \no The government approves the regulatory decisions made by the regulator. \n \no The government organises the political dialogue with social networks by involving the regulator and \ncivil society. \n \no The scope and effectiveness of political dialogue is enhanced by the regulator’s targeted actions aimed \nat promoting the social networks’ accountability. The government is then able to continue its action \nvia political dialogue on all social issues with the social networks by involving civil society (NGOs, \nregions and the educational and academic communities)18. \n \no Central agencies reposition themselves to support the government in its political dialogue by building \non the reduction of the information asymmetry between platforms and political institutions due to \nprescriptive regulations. \n \no Where applicable, the platforms make voluntary commitments to the government, which are subject \nto verification and enforcement by the regulator. For example, the implementation of an action plan \nto tackle a newly identified abuse, improvement of transparency metrics for the coming year, etc. \n \n \n \n                                                            \n18Specifically, the content of the terms of use remains within the social networks’ entrepreneurial freedom, although the \ntransparency provided by the regulator makes them subject to a political dialogue. For example, Facebook acknowledges that it \nhas changed the scope of its community regulations to better protect refugees from hate speech under political pressure. \n\n \n \n \n \np. 23 \nFourth pillar  \n An independent administrative authority,  \nacting in partnership with other branches of the state,  \nand open to civil society \no In the regulatory system proposed, the independent administrative authority guarantees the \naccountability of social networks, for the benefit of the government and civil society. \n \no It implements coercive regulation autonomously but must not be autocratic or hegemonic. It regulates \nthe accountability of the large social network platforms by policing the transparency obligations of \ncontent ordering and moderation systems, as well as the obligation to defend the integrity of the social \nnetwork and its members. It is neither the regulator of social networking services as a whole, nor the \nregulator of the contents that are published on them. It does not have jurisdiction over all contents \ntaken individually. It cooperates with the state agencies, under the authority of the government the \nlegal system. \n \no It does not directly impose restrictions on the definition of the social networking services offered, but \nimposes the publication and dissemination of information, the veracity and relevance of which it seeks \nto qualify with the help of civil society (NGOs, regions and the educational and academic \ncommunities). It verifies the effectiveness of the resources deployed to comply with the obligation to \ndefend the integrity of the social network and its members. \n \no It must have wide-ranging access to information held by the platforms, including the ability to use \nborrowed identities and to require special access to algorithms to verify the accuracy of the \ndescription published by the social network19. It cannot be challenged on the grounds of business \nsecrecy or personal data protection, without undermining its obligation to protect the data it requires \nin accordance with the GDPR and trade secrecy laws.  \n \no It has an administrative sanctioning power enabling it to impose (1) mandatory publicity on the social \nnetwork for these users and/or its commercial partners (i.e. the advertisers responsible for the \nplatform’s turnover), and (2) pecuniary sanctions up to a maximum of [4%] 20of the total global \nturnover of the social network operator. These sanctioning powers may be exercised only after formal \nnotice. \n \no It has the mandate and legal competence to set up links to enable academic research on the platforms \nusing their data, in compliance with GDPR. \n \no It has a mandate to encourage the pooling of resources and knowledge for the benefit of smaller \nsocial networks, thereby contributing to lowering entry barriers. \n \no It actively participates in the European regulators’ network and supports the government’s action in \nthe negotiation of European policy. \n                                                            \n19 Through the implementation of direct, real-time, targeted and proportional access to social network information systems via \ndedicated interfaces (APIs). \n20The mission has not conducted specific work on the level of pecuniary penalties to be set and uses the amounts provided by the \nGDPR.  \n\np. 24 \nFifth pillar  \nEuropean cooperation  \nwhich reinforces Member States’ capacity to deal with global platforms  \nand reduces the risks of implementation in each Member State \nIn view of the power of global platforms, the mission proposes that the European Union must \norganise the networking capacity of governments and their civil societies by joining forces. This \nstructure strengthens Member States’ role as guarantors of social cohesion in a globalised world.  \nThis European level coordination must be based on:  \no A European regulation in order to recognise the global character intrinsic in any digital \nplatform; ensure the full effectiveness of coordinated and networked action by national \nauthorities in front of global players (in particular via common procedures, common \nAPIs, etc.); and reduce the risks of implementation in each Member State. \no National implementation according to the destination country21 rule to make the \nplatforms responsible locally in each Member State and in the regions where they may \ncreate damage. \no Concerted action between national authorities and open to civil societies in order to \nincrease the effectiveness of verification of the platforms’ transparency. \no European mechanisms to reduce the risk of excessive regulation by a Member State \n(\"check and balance\"), an essential corollary of the competing competence of each \nMember State: national and European public consultation on regulatory decisions or \nrecommendations, a mechanism for referral of the opinions of the national personal data \nregulator, coordination and coherence of national regulatory decisions by a collegial \nbody bringing together the national regulators and the European Commission. \n \n \n \n \n                                                            \n21 Member States’ competing jurisdiction should be limited by prerequisites: (1) a threshold expressed as the average number of \nmonthly users as a percentage of the population at national level to establish the regulator's automatic jurisdiction, and (2) a lower \nthreshold when combined with the finding of manifest harm in the destination country, and (3) limited power and penalties in \nproportion to the potential consequences suffered in the Member State. \n\n \n \n \n \np. 25 \nIV –  Focus on the transparency of algorithms \n \nIncreasingly complex algorithms \nUsers of social network experience algorithms every day, from content rating on the newsfeed, to \ninsertion of sponsored content, algorithms to moderate content contrary to the terms of use, friend \nsuggestions, etc. \nWhen dealing with the transparency of these algorithms, it is necessary to take the definition of the word \nalgorithm (“Set of operating rules whose application makes it possible to solve a stated problem using a \nlimited number of operations”), while also encompassing the algorithm's proposed input data, data that \nhas been previously used to “train” the algorithm, the context data, etc. Many algorithms are based on \nstatistical approaches and therefore provide probabilistic answers such as the probability that you might \nlike content or click on a product. Some use machine learning techniques that involve trying to mimic \nhuman choices, for example, the moderators’ choice to accept or reject content grouped into collections \nof annotated content. Most of them are also part highly personalised. Finally, the algorithms also evolve \nsignificantly over time, sometimes updated daily. All of these factors lead to a paradigm shift for the \nintelligibility of algorithms. Each algorithm may have billions of avatars that do not all behave in exactly \nthe same way, depending on the user or the country. Nevertheless, it is necessary for public action to \nextract the general principles. \nA need for algorithmic transparency \nAlgorithms are tools that may be misused or misappropriated. The importance they have gained on social \nnetworking platforms and the abuses they may cause (promotion of hate speech, ineffective moderation, \ninterference by a sovereign state in the public debate, etc.) have made state intervention vital. This \ninvolves transparency, i.e. the means to make the underlying logic intelligible, the main processing \nprinciples applied by the algorithms. This first level, which requires little intervention from public \nauthorities, allows a relative unveiling of the impenetrable workings of certain algorithms. it may also point \nto possible operating biases (whether due to developers’ conscious or unconscious choices, to computer \nprogrammes or to data bias), but above all it fuels the public debate on the social questions raised by the \nwidespread use of algorithms. \nIn real terms, algorithmic transparency takes a variety of forms. For example, for a private individual \nwithout any particular technical skills, it could mean publishing the key criteria that led to a result \nconcerning him or her (information ranking, a recommendation, targeted advertising, etc.) or the reasons \nfor a particular decision (moderation of a post or lack of response following a report). A more expert \noperator will be interested in more comprehensive measurements of algorithms’ performance (false \npositive or negative rate in moderation) or explanations of the processing architecture in the form of \ndecision trees or other graphic representations revealing the data taken into account by the algorithm and \nits influence on the results. The academic world will surely be interested in the publication of reference \ndatasets, making it possible to challenge platforms’ moderation choices, without which it is impossible to \nreproduce the results of a learning algorithm. \nTransparency cannot simply be declared. It is a particularly complex task to check the integrity of the \nalgorithms used by companies. The regulator must have the resources to do this using statistical measures, \nthe provision of API testing tools and third-party certification or compliance mechanisms. \n\np. 26 \nPublic action at two levels \nTransparency will be effective only if it results from regular dialogue with operators and a process of trial \nand error mimicking the development process of those algorithms. The key principles for action by the \npublic authorities therefore clearly need to be defined by legislation in order to impose transparency on \nthe algorithms, while giving flexibility to the regulator responsible for applying the law. The legislator must \ntherefore define the legal principles to be followed by the regulatory authorities while adapting to \nparticular contexts. In practice, it will particularly be necessary to establish a proper equilibrium between \nthe principle of transparency and the protection of business secrecy and to define general obligations of \nintelligibility in respect of the relevant algorithms. It would not be advisable, however, to define particular \nmetrics in legislation or to impose specific implementation procedures. For example, Article 14 of the law \nrelating to combating the manipulation of information is not understood by the sector, which finds it \neither excessively or insufficiently specific (what happens if the algorithm is customised? what happens if \nthe algorithm is updated? what does “share of direct access” mean in the context of a ranking algorithm?). \nSince the law has empowered the regulator to extract information, the regulator is able to study the \nspecific characteristics of each algorithm on a case-by-case basis in order to ensure effective transparency. \nIt is this approach – by definition experimental and involving co-construction and trial and error – that \nwill allow the regulator to develop its policy tools incrementally at a regulatory level. This could then \nsubsequently lead to the emergence of possible synergies to define broader principles governing the \ntransparency of algorithms.  \nBeyond the technical and legal issues surrounding algorithms and their transparency, the aim of regulation \nwill be to bring the ethical issues and moral and political choices raised by algorithms into the public \ndebate, to clearly reveal the “algorithmic policy”. The regulator's accumulated knowledge and the data it \nwill collect will then be able to enhance societal and academic debates better by responding to questions \non the range of issues underlying the concept of transparency, for example regarding: \n \nThe data:  \no What data is used to train the algorithms? How is it collected? Is it personal data? \no What data is used in the algorithm’s input parameters? \no Does this data present biases? \n \nThe model:  \no What is the processing flow followed? What algorithmic components are used?  \no What supervisory/monitoring mechanisms are used in algorithmic learning? \no What personalisation is carried out? \no Does the algorithm reproduce biases? Can it be misused? What are potential abuses to \navoid? \n \nInferences:  \no What are the false positive/false negative rates? \no Which metrics can be used to report on the algorithm’s performance? \no What confidence interval may be applied to the result? \no What procedures are used to correct errors?  \n  \n\n \n \n \n \np. 27 \n \nAppendix 1 \n \n \n \n \n \n \n \n \n \n \nMission letter \n\n\n \n \n \n \np. 29 \nMINISTRY OF THE \nECONOMY AND \nFINANCE \nSTATE SECRETARIAT \nFOR DIGITAL AFFAIRS \n \nMINISTRY OF ACTION \nAND PUBLIC ACCOUNTS \n \n \nMr Frédéric Potier \nPrefect on public service mission, \nDILCRAH \n \nMr Serge Abiteboul, \nInria researcher \nMember of the Arcep College \n \nParis, 11 March, 2019 \n \nRe.: Mission letter – Facebook mission \n \nDear Sirs, \n \nSocial networks now occupy an essential place in our society by offering their users powerful spaces and \ntools for exchanging ideas, discussing and sharing content. In this respect, they provide a fantastic \nopportunity to exercise freedom of expression and freedom of communication, the foundations of our \ndemocratic society.  \n \nWhile they are symbols of progress and spaces for freedom, social networks are also forums for the \ndissemination of unlawful content, which can be seen by a potentially very large audience. Freedom of \nexpression and freedom of communication, like other media, must be reconciled with other principles that \nmay limit the ability to exercise them, including respect for the dignity of the human person.  \n \nThe laws of the Republic obviously apply in the digital space, both in respect of users and platforms. \nHowever, the volume of content, the speed with which it spreads on social networks and its impact on \nsociety justify the complementary implementation of a systemic regulation of moderation systems on \nsocial networks. \n \nDeveloped by social networks on their own initiative, existing moderation tools adopted on a self-\nregulatory basis which, while relevant in some respects, do not provide sufficient guarantees for the \nexercise of the fundamental rights of our fellow citizens.  \n \n \n \n \n \n \n \n \n \n \n \n139 Rue de Bercy - 75572 Paris Cedex 12 \n \n\np. 30 \n \nThese private initiatives can no longer manage without support from the public authorities. This was the \nconviction that led me to entrust you with a mission to investigate and make proposals concerning social \nnetworks’ content moderation systems. The lessons of this mission will complete the conclusions of the \nmission report entrusted by the Government to Ms Laetitia Avia, Mr Karim Amellal and Mr Gil Taieb.1  \n \nIt is in this context that Facebook, taking an experimental approach, has agreed to work with your fact-\nfinding mission and present its moderation system and its development prospects, with particular \nattention to combating the dissemination of content that incites hatred.  \n \nOn the basis of this unprecedented collaboration with a private operator, you will assess these self-\nregulation systems adopted by Facebook. You will particularly study the algorithmic processing developed \nand used by Facebook in this respect.  \n \nYou will propose recommendations, whose risks and opportunities you will have first analysed, and in \nparticular the precautions to be considered in order to build a national regulatory framework which can be \ndeveloped on a larger scale, particularly in Europe, in view of the global nature of social networks. This \nexperiment should be seen as a first step to reflect enabling very specific consideration of the best ways to \nensure that all social networks, not just Facebook, apply very high standards and quality requirements in \nmoderating the content that they host.  \n \nTo carry out this mission, you will consult all stakeholders and take into account any initiatives already \nunder way in other countries.  \n \nYou will draw on a team of experts whose composition is set out in the appendix and on three rapporteurs \nmade available for the duration of the mission. With your agreement, Mr Benoit Loutrel will be the \ngeneral reporter.  \n \nYou will report periodically on the progress of your work to a steering committee made up of the offices \nof the relevant ministers and chaired by my representative.  \n \nThe lessons learned from this experiment will feed into the national and European regulatory work in this \nfield. I wish to have your report available by 30 June 2019 at the latest.  \n \n \nMounir Mahjoubi  \n \n \n \n \n \n \n \n \n1 Report submitted to the prime minister on 20 September 2018, entitled “Strengthening the fight against racism and \nanti-Semitism on the internet”. \n \n \n\n \n \n \n \np. 31 \n \nAppendix 2  \n \nThe members of the mission \n \nSerge Abiteboul \nDirector of IT research at the Inria and the École Normale Supérieure in Paris, member of the Collège de \nl’Arcep, Serge Abiteboul is an expert in databases, information and knowledge. He was a professor \n(Collège de France, Stanford, Oxford University, etc.), a member of the CNNum, and a startupper. He is a \nmember of France’s Academy of Sciences. He is also a blogger and author. \n \nFrédéric Potier \nAfter a career in the Ministry of the Interior and the ministerial cabinet, in May 2017 Frédéric Potier was \nappointed prefect on public service mission and interministerial delegate to the fight against racism, anti-\nSemitism and anti-LGBT hatred (DILCRAH). In this post, he guides the State’s public policy against \ncertain forms of discrimination and hate speech and acts. \n \n \nCôme Berbain \nAs an engineer from the corps des Mines, and with a doctorate in cryptology, Côme Berbain is the State \ndirector for digital technology. His career has alternated between private entities (Orange, Trusted Logic) \nand public entities (Ministry of Defence, ANSSI) in the fields of digital transformation and cybersecurity. \nHe was an adviser to the office of the Secretary of State in charge of digital affairs in 2017 and 2018. \n \nJean-Baptiste Gourdin \nA graduate of the Paris Institute of Political Studies (ENA), Jean-Baptiste Gourdin is department head \nand Deputy Director-General of the Directorate General of Media and Cultural Industries (DGMIC). He \nis a Magistrate at the Cour des comptes and was chief of staff of the CSA President and coordinator of \nthe mission \"Act 2 of the cultural exception\". \n \nJacques Martinon \nA court magistrate, Jacques Martinon started his career as a trial judge. Since 2016, he has joined the \nDepartment of criminal affairs and pardons (DACG) and is head of the Justice Department’s mission to \nfight cybercrime. He is a contributor to the SGDSN’s cyberdefense strategy magazine, and has been a \ntrainer for the INHESJ/IHEDN joint national session on “digital sovereignty and cybersecurity”. \n \n\np. 32 \n \nGilles Schwoerer \nGilles Schwoerer is a gendarmerie officer and worked in a wide variety of State sectors (Army, \nDepartmental Gendarmerie, specialised gendarmeries related to aeronautics) before joining the Centre for \ncombating digital crime (C3N) in the Gendarmerie Judiciary Centre as Deputy Chief of C3N. \n \nAude Signourel \nAfter 7 years at the Ministry of Justice and 3 years in the cabinet of the prefect of Seine-Saint-Denis, Aude \nSignourel joined the Directorate of Civil Liberties and Legal Affairs at the Ministry of the Interior. Since \n2017, she has been legal advisor to the Cybercrime sub-directorate of the Central Directorate of the \nJudicial Police, which hosts the PHAROS platform. \n \n \nThe mission rapporteurs \n \n \nSacha Desmaris \nSacha Desmaris graduated as a lawyer from the University of Paris 2 Pantheon-Assas, and is now is head \nof the Audiovisual Economics Department at the Directorate of Studies, Economic Affairs and \nForecasting at the Higher audiovisual council (CSA) where she was a mission head from 2016 to 2019, \nafter having worked four years at the general secretariat of the M6 Group. \n \nPierre Dubreuil \nPierre Dubreuil obtained his engineering degree from Telecom ParisTech and the École Normale \nSupérieure at Paris-Saclay, and is a specialist in machine learning and co-founder of a startup; he is a \nmission head within the Telecommunications and Postal Regulatory Authority (Arcep). \n \nBenoît Loutrel \nAn Inspector General of the INSEE, Benoit Loutrel specialised in industrial economics and regulation, \nand was appointed Director General of the Telecommunications and Postal Regulatory Authority (Arcep) \nfrom 2013 to 2016, after having worked there from 2004 to 2010. He was director of the \"digital\" \nprogramme on investments for the future from 2010 to 2013. He was director of public affairs for Google \nFrance for a few months in 2017. \n \n \n\n \n \n \n \np. 33 \n \n \n\np. 34 \n \n", "published_date": "2019-05-11", "section": "Dossiers"}