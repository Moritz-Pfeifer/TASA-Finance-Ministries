{"id": 5556, "url": "https://minefi.hosting.augure.com/Augure_Minefi/r/ContenuEnLigne/Download?id=A217285E-A55D-47BF-B924-4435CB8506AA&filename=18%20-%20Mission%20Re%CC%81gulation%20des%20re%CC%81seaux%20sociaux%20-ENG.pdf", "author": null, "title": "18 -Rapport de la mission de régulation des réseaux sociaux - Version anglaise", "text": " \n \n \n \n \n \n \n \nCreating a French framework  \n \nto make social media platforms more accountable:  \n \nActing in France with a European vision \n \n \n \n \n \n \n \n \nMission report \n \n \n \n \n“Regulation of social networks – Facebook experiment” \n \n \n \n \n Submitted to the French Secretary of State for Digital Affairs \n \n \n \n \n \n \nMay 2019 \n \nVersion 1.1 \n(This second release of the English version of the report corrects various translation issues) \n \n\np. 2 \nExecutive Summary \n \nSocial networks allow any member of society to publish any content they wish and share it with other users \nof the network. They are thereby revolutionising the media industry and communications by offering \nindividuals and civil society a direct means of expression. It is no longer necessary to use conventional media \nto communicate publicly. Using social networks therefore considerably increases individuals’ ability to \nexercise their freedom of expression, communicate and obtain information. \nNevertheless, the opportunities offered by social networking services can lead to unacceptable abuses of \nthose same freedoms. These abuses are being committed by isolated individuals or organised groups to \nwhich the leading social networks – including Facebook, YouTube, Twitter and Snap, to cite just the largest \n– are not providing an adequate response. Yet through their ordering of published content and moderation \npolicies, social networks have the ability to take direct action against the worst abuses to prevent or respond \nto them and thereby limit the damage to social cohesion. \nPublic intervention to force the biggest players to assume a more responsible and protective attitude to our \nsocial cohesion therefore appears legitimate. Given the civil liberty issues at stake, this intervention should \nbe subject to particular precautions. It must (1) respect the wide range of social network models, which are \nparticularly diverse, (2) impose a principle of transparency and systematic inclusion of civil society, (3) aim \nfor a minimum level of intervention in accordance with the principles of necessity and proportionality and \n(4) refer to the courts for the characterisation of the lawfulness of individual content. \nThe current approach of self-regulation of social networks is interesting, as it demonstrates that platforms \nmay be part of the solution to the problems observed. They have come up with varied and agile solutions, \ne.g. removal, less exposure, reminder of common rules, education and victim support. But self-regulation is \nstill evolving, remains too reactive (after the appearance of harm), and lacks credibility due to the extreme \nasymmetry of information, which gives rise to a sense of “story-telling” which nourishes suspicion about \nthe reality of the platform’s actions. \nThe public policy response must find a balance between a punitive approach, which is vital for sending a \nstrong political signal to the perpetrators of abuses, and the approach of making social networks increasingly \naccountable through preventive regulation, capitalising on platforms’ capacity for self-regulation. \nGiven the unique and ubiquitous nature of social networks, which transcend the borders of Member States \nand offer a unique service in different areas, this ex-ante regulation must be adopted and implemented at \nEuropean level. The current “installation country” approach – according to which only the country in which \nthe social network’s headquarters is based can intervene to regulate this network – has proven inefficient. \nThe damage caused by the excesses and abuses of social networks to social cohesion in destination Member \nStates is difficult to observe from the installation Member State.  \nAny French initiative should therefore aim to reverse the current European approach to focus instead on \nthe destination country, in which the platform is responsible to the Member State where the damage has \noccurred. This would strengthen each Member State’s ability to address the consequences of globalisation. \nThis objective must be taken into account when designing a regulatory function for social networks so that \nthe solution appears relevant for our main European partners, even though the policy on regulating media \nindustries differs significantly from one state to another.  \nThe implementation of an ex-ante regulatory function should respect three conditions: (1) to adopt a \ncompliance approach, according to which the regulator supervises the correct implementation of preventive \nor corrective measures, but does not focus on the materialisation of risks nor try to regulate the service \nprovided, (2) to concentrate on the systemic actors capable of creating significant damages to our societies, \nwithout creating entry barriers for new European operators, (3) to stay agile to confront future challenges \n\n \n \n \n \np. 3 \nin a rapidly evolving digital environment. Legislative measures should therefore aim to create an institutional \ncapacity to regulate rather than a regulation specifically applicable to current problems. \nThat regulation could be based on the following five pillars: \nFirst pillar: \nA public regulatory policy guaranteeing individual freedoms and platforms’ \nentrepreneurial freedom. \nSecond pillar: \nA prescriptive regulation focusing on the accountability of social networks, \nimplemented by an independent administrative authority and based on three \nobligations for the platforms: \n• \nObligation of transparency of the function of ordering content, \n• \nObligation of transparency of the function which implements the Terms of \nService and the moderation of content, \n• \nDuty of care towards its users. \n \nThird pillar:  \nInformed political dialogue between the operators, the government, the \nlegislature and civil society. \nFourth pillar:  \nAn independent administrative authority, acting in partnership with other \nbranches of the state, and open to civil society. \nFifth pillar:  \nA European cooperation, reinforcing Member States’ capacity to take action \non global platforms and reducing the political risks related to implementation \nin each Member State. \n\np. 4 \n \nForeword \n \nThe mission’s objective was to explore a general framework for the regulation of the social networks, starting \nfrom the fight against online hatred and relying on the voluntary cooperation, outside any legal framework, \nof Facebook1 (see the appended mission letter).  \nThe purpose of this experiment is to explore how a new system to regulate social networks could be \nestablished to complement existing instruments and better achieve public policy objectives in terms of the \nreconciliation of public freedoms and the safeguarding of public order on social networks. Although the \nexchanges with Facebook thus focused on hate content, the mission’s conclusions may be applied to all the \nissues raised by the publication of content on social networks. \nThis interministerial mission team2 comprises seven high-level experts and three permanent reporters from \na range of ministries – Culture, Interior, Justice, Economy, Prime Ministerial services - DILCRAH3 \n(Interministerial Delegation to Combat Racism, Antisemitism and Anti-LGBT Hate), DINSIC4 \n(Interministerial Delegation of Digital and Information and Communication Systems) and independent \nadministrative authorities - ARCEP5 (electronic communications and postal authority) and CSA6 \n(audiovisual regulatory authority).  \nThe mission worked with Facebook throughout January and February. Over the course of several working \ndays with the mission in Paris, Dublin (location of its European headquarters) and Barcelona (location of \none of the moderation centres), Facebook’s representatives presented its policy for moderating hateful \ncontent, its organisation, and the resources it devotes to this as well as its internal procedures. Meetings \nwere held to examine specific topics in depth, including the use of algorithms in the moderation system to \ndetect hateful content and the basic principles of algorithms that order content for Facebook users. \n \nAlthough the mission received a very open welcome from Facebook, it did not have access to particularly \ndetailed, let alone truly confidential information. This was due to the speed of the work, the lack of a formal \nlegal framework and the limits of Facebook’s transparency policy. The mission is nevertheless convinced \nthat this limitation did not affect its results, as its goal was not to evaluate the relevance of Facebook’s \nmechanisms, but to imagine “rules of the game”7, which could be adopted by the legislator to create a long-\nterm regulatory framework for global actors operating abroad, such as Facebook.  \nIn this respect, the report does not detail Facebook's mechanisms for moderating the fight against the \ndissemination of hateful content online. Nevertheless, the reader can refer to documents published by \nFacebook to better understand the mechanisms for moderating the social network and in particular the \ncommunity standards, the report on the transparency of content management on its platform, and the \"hard \nquestions blog” on which Facebook regularly publishes reflections on the subject of moderation (in \nEnglish). \n                                                           \n1Following an agreement between the company’s Chairman, Mark Zuckerberg, and the President of the Republic announced at the \nInternet Governance Forum in November 2018. \n2The composition of the mission is appended to this report. \n3 Délégation interministérielle à la lutte contre le racisme, l’antisémitisme et la haine anti-LGBT is France’s Inter-ministerial \ndelegation for the fight against racism, antisemitism and anti-LGBT hatred. \n4 The Direction interministérielle du numérique et du système d'information et de communication de l’État is the Interministerial \nDirectorate for Digital Technology and the Government Information and Communications System. \n5 The Autorité de régulation des communications électroniques et des postes regulates France’s telecommunications. \n6 The Conseil supérieur de l’audiovisuel regulates France’s electronic media. \n7The mission is convinced in this respect that the regulators’ first power should be its right to demand the communication of any \ninformation necessary for the accomplishment of its mission in a legally enforceable framework. \n\n \n \n \n \np. 5 \nThe mission also presented its approach to associations fighting hate speech, at a seminar organised by \nCNNum (National Digital Council) on 14 February and 15 February 2019.  \nThe mission finally completed its work with a study trip to Berlin to better understand the experience of the \nGerman NetzDG law, a mission in London and a series of meetings with public operators – Inria, Platform \nPharos; Centre de lutte contre les criminalités numérique, Secrétariat général aux affaires européennes, \nConseil national du numérique, Direction générale des entreprises, Direction générale du Trésor), and \nprivate entities and NGOs (Reporters sans frontières, le CERRE, la Quadrature du net, Webedia, Netino, \nSnap, Google, and Twitter). \nThis report formulates proposals which, if adopted, need to be fleshed out. This report (1) identifies some \nkey features of social networking services, (2) analyses some public policy approaches to those services and \n(3) recommends creating a new regulatory system based on five pillars and (4) presents a focus on the \nconcept of the transparency of algorithms and its implementation. \nDue to scheduling constraints, several topics were not examined. In particular, the mission did not conduct \na study of the competitive impact of the proposed regulatory scheme on other social network service \nofferings. However, the regulatory system should be careful not to create an insurmountable entry barrier \nfor mid-sized market players or new entrants, and, consequently, to unduly favouring the consolidation of \nthe hegemonic actors by enacting regulatory barriers. \nFurthermore, the mission focused its study on public content, but it is clear that hateful content is also \npresent in private or closed groups on social networks, and that there is currently a trend for increasing \ndissemination of content within these limited groups and on messaging services. It is more complex to \nintervene on these environments where exchanges can be covered by the secrecy of private correspondence \nand especially encrypted \"from end to end\", rendering illusory any moderation by the platform itself since \nthe content exchanged between the users has no visibility. \nFinally, the report does not deal with intervention methods for “non-cooperative” social networks which \ndo not correspond to a traditional economic rationale, whether they are militant extremists (4chan, 8chan, \netc.) or controlled directly or indirectly by a sovereign state pursuing political objectives. \n \n \n\n\n \n \n \n \np. 7 \nI –  Social networking services \n \n \nBy enabling everyone to publish content and share it with other users, social networks are revolutionising \nthe media industry and communications by offering individuals and civil society a direct means of \nexpression. Nevertheless, the possibilities offered by social networking services give rise to unacceptable \nabuses by isolated individuals or organised groups, to which the operators are not providing a sufficient \nresponse or are even contributing to via their content ordering systems.  \n \n1.1  \nAlthough the purpose of all social networking services is to share and disseminate content \nto the public online, they are nevertheless very diverse  \n \nSocial networking services are defined by the ability to disseminate content produced by their users to all or \nsome of the other users on that network. \n \nSocial networking services are provided by different types of operators, differentiated by their legal status, \ntheir type, their target, their economic model, the type of content published8 and their distribution methods. \nThis falls into two categories: \n- \nSocial networking services offered on an ancillary basis: Thematic or general discussion forums on \nwebsites (e.g. jeuxvideo.com and comment spaces on media websites such as lemonde.fr or \nlefigaro.fr) constitute a basic form of a social network: content organisation is rudimentary (mainly \nby chronological order) and, when monetised by advertising, this is usually not combined with user \ncontent, but is adjacent; \n- \nSocial networking services offered as the main focus: Social networking platforms, which may be \ngeneral, like Facebook or Twitter, or structured around a particular content type or format, like \nYouTube (videos), Pinterest (photos), TikTok (short videos) or Snapchat (short videos and photos). \n \nAll of these services offer some or all of the following content and features: user-generated content (UGC) \nas the main content, promotional content9, content from professional publishers, content accessible to \neveryone and/or content which is restricted to a select group of users, individual accounts with or without \na screen name, or discussion areas attached to a community or an event.  \nThe content ordering system on a social networking service may be personalised (i.e. specific to each user) \nand be more or less sophisticated depending on the volume of published content. \nPlatforms increasingly offer a private messaging service with social networking services: Direct message for \nTwitter, Messages for YouTube, Messenger (and WhatsApp) for Facebook. In some cases, as with Snapchat \nfor example, content dissemination to a closed group may be the default option, but the user may make the \ncontent available to the entire network at any time.  \nMonetisation methods vary widely from one service to the next, from user-independent advertisements and \nshared content, to advertisements targeted according to the user’s favourite content or targeted according \nto the user viewing the content. Revenues may be shared with the content publisher, as is the case for \nYouTube and Snapchat. \n                                                           \n8 The content may be a text, a hyperlink, an image, a sound, a video (sometimes in real time), a computer programme and/or any \ncombination of these six elements. \n9 This may be traditional advertising, a product placement, sponsored content, etc.  \n\np. 8 \nBeyond the distinction by type of operators and functionality, the size of the social network is a central \ncriterion to take into account: from 2 billion users for Facebook, with a worldwide presence, to a few \nthousand users on some discussion forums.  \n \nFinally, models and uses are not static and in fact evolve very quickly. Services generate new uses while, \nconversely, user behaviour is constantly inspiring operators to adapt their services.  \n \nA first attempt at a legal definition of a social networking service was first introduced in the bill proposed \nby French MP Laetitia Avia, designed to combat online hatred. The definition of operators of online \nplatforms set out in Article L.111-7 of the French Consumer Code isolates operators of online platforms \n“offering an online communication service to the public based on connecting several parties in order to share public content”.10 \nAt European level, the Audiovisual Media Services Directive incidentally introduces the concept of “social \nmedia services”. Its Article 1 defines “audiovisual media services” and it states, in recitals (4) and (5), that \nsocial audiovisual media services are those whose content is created by users.11  \n \nA social network may be defined as an online service allowing its users to publish content of their \nchoice and thereby make them accessible to all or some of the other users of that service.  \n \n \n \n1.2 Social networks are revolutionising the media industry and communications by offering \nindividuals and civil society a direct means of expression. In that sense, they represent a great \nstep forward for freedom of expression  \n \nWith a capacity to host and distribute mass content for a very low marginal cost, social networks are a new \nform of media enabling direct expression, without pre-selection of authors or content, or any journalistic intermediation. A \nsocial network allows the exchange of content that it has neither created nor pre-selected, subject to \ncompliance with rules of publication issued by the social network (see below). This lack of creation or \nselection, which distinguishes social networks from traditional news media, allows everyone to express themselves, \nto publicise and disseminate their opinions or content of their choice and to access new sources of information. The \nability of an individual, an association or a private or public operator to express itself publicly is no longer \ndependent on the editorial choices of traditional media. \nThese networks are creating new forms of social relations, transcending geographical limitations (and even \nlinguistic limitations as a result of translation tools) and subverting both historical social structures and the \nprimacy of the territorial organisation of states and our societies. New “digital” associations, intangible yet \nvery real communities, have sprung up to share information or areas of interest or to unite around a common \ncause.  \nThe opportunities offered by these new communication vehicles are reflected in user behaviour. Social \nnetworks are now vital tools for accessing and disseminating information. One-third of French people and \nhalf of 18 to 24-year-olds obtain their information from social networks, while video-sharing platforms \nrepresent half of all news videos watched on the internet.12  \n                                                           \n10 It is marked by the definition set by German legislators: the German NetzDG, adopted in 2017, defines the social networks of \nfor-profit internet platforms, which is intended to allow users to share any content with other users or to make that content \naccessible to the public. Platforms managing editorial or journalistic content are excluded. Law no. 2018-1202 of 22 December \n2018, relating to combating the manipulation of information, targets all operators of online platforms within the meaning of the \nFrench Consumer Code.  \n11When video sharing is an “essential feature”, those services fall into the category of “video sharing platforms”, distinct from that \nof “audiovisual media services” and subject to simplified regulations mainly intended to protect young users and tackle the \ndissemination of hate content. \n12 Reuters Institute, Digital News Report 2018.  \n\n \n \n \n \np. 9 \n \n1.3 Social networking services define content ordering and therefore exert a form of de facto rather \nthan legal editorialization, which is generally unobservable and non-transparent  \n \nAll of the content published on a social network cannot be presented to users without ordering. The volume \nof content published necessarily requires the platform to define an order to display and to carry out a \nselection, without prejudice to the user's ability to search for specific content if they so choose. The content \nwhich the user will actually view will depend firstly on the layout of his or her interface and the use of \nalgorithms to prioritise and personalise presentation of the various content. Unlike traditional media, the \nordering of content on social network services is usually personalised (except in forums) and everyone sees \nthe result of the personalisation when accessing the service. However, the overall effects of this ordering on \nall users are not observable.  \nFurthermore, the operators providing social network services do not always reveal the precise criteria used \nto define the presentation of content. These ordering criteria may be numerous, and their weighting varies \naccording to the purpose of the service (supposed interest of the content, identity of the author, whether \nthey are paid-for, user’s preferences and behaviour, etc.). More generally, the ordering function gives \noperators of social network services the capacity to accelerate or, on the contrary, slow down the dissemination \nof certain content. \nThe existence of this function of ordering content, constituting a form of de facto editorialization, cannot \nquestion the legal status of the operators or lead to legal requalification of hosting providers as publishers, \nsince the majority of social network services do not carry out any selection prior to the publication of \ncontent. \n \nThe existence of this capacity to organize the information plays a key role in the dissemination \nof contents and in social networks’ ability to prevent or increase damage to social cohesion. \n \n1.4 The freedoms of communication and public expression offered by social network operators \nlead to unacceptable abuses by isolated individuals and organised groups to which the social \nnetworks are not providing an adequate response \n \nWhether paid-for, free or paying, the majority of content published on social networks does not pose any \ndifficulty13. As a result of this capacity for large-scale communication and expression, however, combined \nwith a feeling of relative anonymity and impunity, social networks are also forums for the exchange of \nunacceptable content and behaviour (content inciting hatred, terrorist content, child pornography, online \nharassment and identity theft) which can have a significant impact on social cohesion and harmony \n(spreading of false information and unfounded rumours, attempts to fraudulently manipulate public opinion \nby individuals or groups with political or financial objectives).  \nMost operators have implemented terms of use which indicate the categories of content which are accepted \non the service as well as moderation mechanisms when those rules are not respected by users. Given the \nvolume of content published and the statistical approach taken by algorithmic tools, however, social network \noperators are currently unable to prevent all risk of their services being abused. In fact, the efforts deployed \nare still largely perfectible, especially by those with a large audience. In addition, little information is made \npublic on how terms of use are defined and implemented or how the moderation system works. \n                                                           \n13 Out of 10,000 Facebook content views, for example, between 23 and 27 apparently contain scenes of explicit violence (Facebook’s \ntransparency report, figures for Q3 2018).  \n\np. 10 \nThe fight against the dissemination via social network services of harmful content to users and social \ncohesion involves looking at how rules are defined, moderation of content already posted and, potentially, \ntheir ordering system, particularly if it involves the personalisation of content.  \n \nEven if the abuses are committed by users, social networks’ role in the presentation and selective \npromotion of content, the inadequacy of their moderation systems and the lack of transparency of \ntheir platforms’ operation justify intervention by the public authorities, notwithstanding the efforts \nmade by certain operators. \nThe development of public policies designed to prevent abuses and misuse of social networks \ntherefore appears necessary but should be subject to particular precautions in several respects.  \nFirstly, it will be necessary to take into account the diversity of operators providing these types of \nservices and, at least initially, to concentrate on those with the most influence over our societies.  \nSecondly, any state intervention must be strictly necessary, proportionate and transparent \nwhenever it affects public freedoms that are as important as the freedom of expression and freedom \nof communication.  \n \n \n\n \n \n \n \np. 11 \nII –  Promoting a new public policy approach  \n \nThe inadequacy and lack of credibility of the self-regulatory approach adopted by the largest platforms \njustify public intervention to make them more responsible. That intervention must be based on a balance \nto be defined between the punishment of authors of harmful content and pragmatic and flexible ex ante \nregulation of operators providing social networking services, within a revised European framework.  \n \n \n2.1 \nPlatforms that are using self-regulation with partial results \n  \nThe work carried out with Facebook, supplemented by discussions with other operators, show that the \nplatforms are striving to develop a self-regulation approach. \nIn the case of Facebook, the mission found that the system has self-regulatory mechanisms endowed with \nincreasing dedicated resources:  \n• \nrecent transparency on the detailed content of “community standards”;  \n• \nincrease of human resources and development of mass processing algorithms dedicated to the \nmoderation system; \n• \ncurrent development of “distributed” moderation tools available to users; \n• \ninternal organisation of the function of moderation, publication of transparency reports; \n• \nattention to the establishment of open governance structures extending beyond platform \nrepresentatives, in particular a supervisory board made up of independent experts, responsible for \nreviewing moderation decisions. \n \nAs for YouTube, the self-regulatory approach of Google’s video-sharing platform includes tools to educate \nusers of the platform in prohibited behaviour or if they are the victims of the aggressive behaviour of other \nusers. For example, the platform has employed influencers to try to change user behaviour, especially among \nthe youngest of them. However, the effectiveness of what seem a priori commendable initiatives remains to \nbe assessed.  \nIn addition, the mission was able to observe that the moderation system not only involves the removal of \ncontent considered toxic, but that there is a range of possible responses, depending on the type of content \nand the degree of potential damage it could cause (quarantining, hiding with a prevention message, de-\nreferencing, warning, etc.). \n \nThe speed of deployment and progress made during the last 12 months by an operator such as \nFacebook show the benefits of capitalising on this self-regulatory approach already being used \nby the platforms, by expanding and legitimising it.  \n \nThe self-regulatory capacity observed at these operators providing a social network makes it \npossible to position them as key elements in the solution to social cohesion issues raised by the \npresence of certain content on these platforms.  \n \nThis solution cannot be reduced to simply removing obviously illicit content, but must be \nenhanced in order to avoid harm (prevention) and respond in all possible situations based on \ntheir severity and the risk to users: quarantine, deceleration, demonetisation, reminder of the \ncommunity rules, targeted education, etc. \n \n \n\np. 12 \nNevertheless, even anticipating the full effect of operators’ stated ambitions, the mission found that it would \nnot offer a sufficient response to public policy concerns: \n• The extreme asymmetry of information between social network operators, on the one hand, \nand public authorities and civil society, on the other, considerably undermines the \ncredibility of a self-regulation approach. \n \nNeither the public authorities nor civil society know how much credence to give to operators’ \nstatements. They have access to practically the same level of information as a user. None of the \ninformation made public by the platforms concerning their self-regulatory actions can be \ncorroborated by objective facts. This limitation is inner to the functioning of the main social \nnetwork services, due to the personalisation of the content provided. Creating an account on these \nplatforms allows users to see only a tiny fraction of them. Only the platform itself is able to measure \nimpacts at a global scale.  \n \nThis lack of credibility is heightened by the enormous volume of content and number of users of \nthe platforms, necessarily requiring processing by algorithms based on a statistical approach. Being \nunable to prove the existence of a systemic failure by the platform, the public authorities and \nrepresentatives of civil society are reduced to highlighting individual examples of unmoderated or \npoorly moderated content. Yet these isolated failures are insufficient to prove a potential systemic \nfailure. \n \n \nThe persistent dissatisfaction of the public authorities can be explained in particular by \ntheir inability to assess the measurable reality and value of the self-regulation carried out \nby these operators, due to a lack of information validated by a trusted third party. \n \n \n• Self-regulation remains too “inward-looking” \n \nNo doubt because of its lack of maturity, self-regulation remains unconvincing because social \nnetwork operators hold all the cards: they draw up their terms of use, decide to what extent to be \nbound by them, modify them as necessary without any public formalities, interpret them without \nthe possibility of appeal and report on their implementation in the form and frequency they \nconsider appropriate.  \n \nDue to their recent, competing and disparate nature, social network services have each developed \ntheir own model of self-regulation. The minimum level of credibility normally provided by a \nsectoral approach, which allows for “peer review” – e.g. the approach taken by the ARPP (French \nAdvertising Regulatory Authority) – is absent here and does not seem to be contemplated. \n \n• Self-regulation is agile but is not subject to any form of supervision \n \nSocial network platforms are agile. They have developed with an entrepreneurial spirit which, \nsometimes deliberately, disregards certain regulatory constraints in order to preserve their ability to \ninnovate. They constantly test the efficiency and relevance of their user interface, their algorithms \nand the organisation of their moderation function, particularly using A/B testing14. Voluntarily \ngiving up, even partially, what has been their main strength therefore remains a major challenge for \ntheir management teams.  \n \nToday, it may well be argued that the major platforms are developing a self-regulatory approach \nnot in order to assimilate and fully address general public policy objectives, but rather to contain \nany risk of coercive intervention by the public authorities and pressure from civil society, in order \n                                                           \n14 Technique involving testing a feature, editorial, graphical interface or new algorithm on two different groups of users to assess \nits effects.  \n\n \n \n \n \np. 13 \nto avoid damage to their reputation. In this context, all of the initiatives taken, however relevant, \nlack credibility and are difficult to assess. \n \nToday, as far as the mission is aware, no social network has adopted truly enforceable rules in terms \nof providing users with information about terms of use, processes to amend them or mechanisms \nto involve civil society or the public authorities in their development, even in an advisory capacity. \nThe credibility of the self-regulatory approach clearly suffers as a result.  \n \nAn approach that puts the social networks at the heart of the regulatory model seems quite \nrelevant. In this model, the social network platform incorporates public interest objectives, \nmodifies its organisation, adapts itself to this “social” objective and acts either upstream at the \ndesign stage, to prevent difficulties, abuses and other misuse of its service, or downstream, to \naddress unacceptable behaviour by its users.  \n \nTo borrow the GDPR term “Privacy by design” relating to personal data, we could speak of \n“Accountability by design” for the processing of content by the social networks. \n \n2.2 \nDeveloping a public policy to make the platforms more accountable  \nThe aim of the public intervention model is therefore not to regulate activity, i.e. impose functional or \ntechnical constraints on the services provided, but to make the social network operators more accountable \nby a legally binding obligation to come up with resources and to be report upon it. Such a model, insofar as \nit minimises public interference in the functioning of a media industry whose core purpose is to serve as a \nmedium for individual expression, would also have the virtue of minimising criticism concerning the risk of \nthe manipulation of information by the public authorities. This criticism is inherent to the industry’s \npurpose. It should not deter public intervention, but it does call for special precautions. \nMore direct regulatory interventions, such as those in the energy, transport, telecommunications, the \ntraditional audiovisual media and online gambling industries, would also seem to be less appropriate, since \nthey involve activities clearly attributable to a given national territory and therefore to the jurisdiction of a \nsingle regulator. However, the social networks often transcend geographical national borders. A discussion \nof content written in French inciting hatred of refugees, published by a user located outside the European \nUnion, may be the subject of comments, also potentially hateful, by a set of other French-speaking users \nlocated anywhere in the world. \nAn intervention method using co-regulatory mechanisms that imposes the internal assimilation \nof public interest objectives, without defining the methods, would make it possible to limit the \nimpact on social network services.  \nThis new method of public intervention, focused on creating a duty of care from the social \nnetworks towards its members, on the one hand, and on improving the credibility of self-\nregulation, on the other, would not undermine the founding principles of social networks, in \nother words their unique, ubiquitous and agile nature. \n \n2.3 \nA public policy dynamic that must find a balance between a punitive approach and \nmaking social networks increasingly accountable through preventive regulation \nIn several European countries, the initial public policy response to issues identified on social networks has \nbeen to implement or strengthen punitive sanctions targeting the authors of content deemed unlawful as \nwell as the platforms, which, because they display and host the content, appear to bear the same liability as \nthe author, or at least to be “accomplices”. \nThe punitive policy is necessary in that it expresses the rules adopted by society in a clear and visible way. \nIt is also required on a purely political level in situations of manifest disruption of public order. The punitive \n\np. 14 \npolicy is effective only if it is comprehensible and enforced, so as to avoid any feeling of impunity for the \nauthors of unacceptable content. \nThe punitive policy, in particular because it opens up the possibility of recognition of harm and its \ncompensation, is an essential tool, but it cannot achieve all public policy objectives. It is limited by the fact \nthat it necessarily intervenes ex post, to sanction unlawful behaviour recognised as such by a court. The \npowers devolved to criminal and civil authorities and the legislative timetable currently make it impossible \nto anticipate changes in social networks and the disruption they can cause. \nUnlike traditional media, social networks do not select each item of content published on the service. This \nis a defining characteristic of such services. Punitive measures against them therefore raise several \ndifficulties. The social network finds itself in the position of a censor, ex post, of all users’ posts on its \nnetwork, essentially after these are signalled (using the platform’s interface) or notified (LCEN’s specific \narrangements) by users or the public authorities. By imposing an absolute standard of conformity that does \nnot take into account the volume of the published content, the audience or the statistical nature of the \nprocessing, punitive measures risk encouraging over-moderation and thereby infringing freedom of \nexpression, which is constitutionally and conventionally protected. \nMoreover, the punitive approach requires the platforms to judge the manifest lawfulness of a content \nthemselves. They consider that this lawfulness is particularly difficult to assess from the triple perspective \nof the legislative intention, prosecution practices – which are by definition more selective depending on the \nchances of prosecution – and case law of national and international courts, which is based on balancing \nfreedom of expression against public order imperatives. To the best of our knowledge, the establishment of \n“guidelines on manifestly hateful content” by an administrative authority, even an independent one, does \nnot seem to be a very satisfactory solution. \nLastly, the scope of content that is not “manifestly unlawful” (grey zone) varies according to geography and \ndoes not easily lend itself to European or international harmonisation, particularly when it comes to content \nthat could be qualified as inciting hatred, because of its historical, cultural and legislative associations specific \nto each state. Particularly since punitive measures are often implemented or strengthened after a crisis \narousing strong public opinions and calling for, and authorising, a strong political response. Except in \nexceptional cases, these crisis situations are local and do not cross borders (or to a minimal extent). The \nconditions for supranational harmonisation of punitive responses are therefore very difficult to meet. \nPunitive measures should be supplemented by the adoption of a second public policy \ncomponent designed to make platforms more accountable by establishing an obligation of \ntransparency and creating a duty of care towards its members, by creating targeted and \ncomprehensible incentives. \n \n2.4 \nEuropean cooperation to be reviewed \nAt the current stage of European integration, social cohesion is primarily established at the level of Member \nStates. Although services are global, the damage resulting from their existence occurs at a national level. In \nEurope, due to the different languages in use, communities on social networks are formed on the basis of \nlinguistic affiliation and most frequently on a national or sub-national basis. \n \nThe accountability of social network operators should be organised at the level of Member \nStates, which are more directly affected by abuses, rather than at the level of the European Union \nitself, which remains removed from crises and their consequences in terms of public order and \nsocial harmony. \n \n\n \n \n \n \np. 15 \nAs a result of the coherent legal space it offers, the European Union nevertheless is in a unique position to \nallow Member States to act coherently in respect of global and geographically ubiquitous operators. The \nEuropean Union offers the ability to bring the combined weight of Member States to bear against the power \nof the large social networking platforms when those states adopt the same standard of regulation.  \nThe European Union is also in a strong position to reduce the risks of failure or excessive regulation by \npublic policies, by reducing political risk at the level of each individual Member State. This capacity has \nparticularly been demonstrated in the telecommunications sector, where the transformation of public \nmonopolies into a competitive industry has benefited significantly from the European capacity to moderate \noccasionally excessive regulatory decisions and to overcome national inertia.  \nNevertheless, the opportunities to take advantage of European construction require a new ambition:  \n• \nThe dialogue between the operators, the Member States and the European Commission on the \nmonitoring of the self-regulation of illegal content seems to be bearing fruit in light of the results \nof the fourth evaluation of the EU Code of Conduct on combating illegal online hate speech, \npublished last February15. Nevertheless, there is undoubtedly room for progress: this initiative \nsuffers from the great distance between the European level at which implementation occurs and \nthe location of the damage. Moreover, although the commitments provided for in the framework \nof this approach are relevant, the voluntary nature of the commitments, without any penalty \nmechanisms, is revealing its limitations (see above); \n \n• \nthe legal regulatory framework was built around the principle of the country of origin, giving \nexclusive regulatory responsibility to the Member State in which the service is established. The \nability of each Member State, apart from the one hosting the service, to tackle any mistakes by a \nglobal player is therefore drastically reduced by this European cooperation, therefore increasing the \npolitical risk in the destination country (in which the damage is produced). Yet the authorities of \nthe country where a breach by the service was identified are those best placed to establish and \ncorrect that breach, especially when it involves assessing an abuse of freedom of expression (e.g. \nthe publication of hate content) which must be assessed in light of the social, political, cultural and \nhistorical context of the state affected. Moreover, the political risk between Member States is also \nsimultaneously increased since a single Member State receives the exclusive benefit from the \nplatform's establishment on its territory, reducing its incentive to intervene in the event of breaches \nby a platform while other Member States suffer the potential damage and remain powerless to act.\n \n \n \nThis structure, based on the jurisdiction of the country of origin, is reflected in the Audiovisual \nMedia Services Directive and in the version of the draft regulation to combat the dissemination of \nterrorist content adopted by the Council of Member States last December. \n \nThe establishment of a European regulation based on the principle of jurisdiction of the \ninstallation country would strip the Member States of their ability to take action against the large \nsocial network platforms present on their territories. \n \nThe establishment of national regulations by each Member State would produce a high risk of \nincompatibilities between those regulations due to the unique and ubiquitous nature of social \nnetworking services, which would then become subject to contradictory and therefore ineffective \nrulings. These national systems would be exposed to the risk of non-compliance with treaties.  \n \n                                                           \n15 http://europa.eu/rapid/press-release_IP-19-805_fr.html \n\np. 16 \nIn contrast, cooperation based on a European regulation and the principle of the “destination country” \ncould reinforce each Member State's capacity to cope with the difficulties generated by global players such \nas the largest social networks, while reducing the political risk for those players: \n• \nBy organising the platform's accountability according to the destination country, it creates an \nincentive for each social network to prevent abuses in each region. This restores geographical \ncoherence between the location of the regulatory function and the location of the damage; \n• \nA common framework, laying down uniform obligations, defined at European level via a directly \napplicable regulation, would guarantee the consistency and uniformity of the legal standard across \nall regions. This is essential in respect of global players enjoying practical ubiquity. Each Member \nState becomes responsible for implementing a common rule within its territory. This enables \ncoordinated action by Member States and their regulatory authorities. It also allows the \nestablishment of mechanisms to mitigate the political risk for each Member State at a European \nlevel using various proven mechanisms, including peer review, establishment of a board of \nregulators, direct supervision by the European Commission and judicial review by the ECJ. \n \nThese structures have already been tested and implemented in respect of the regulation on net neutrality in \nthe European Union. \nIn the absence of such a mechanism, and in view of the issues at stake, Member States risk unilaterally to \nlaunch legislative initiatives with disparate scopes and obligations to the detriment of the digital single market \napproach and to the benefit of existing players which would be the only ones able to support the economic \nburden of a series of national regulations (thereby encouraging a “winner takes all” outcome). \nThe challenge of setting up a French regulatory framework for social networks must be viewed \nin light of its ability, under the new EU presidency, to serve a proposal to: \n• \nreverse the current European trend, move away from the logic of the installation country, \nwhich weakens the sovereignty of Member States and their capacity to tackle \nglobalisation, and switch to a destination country approach in order to strengthen the \nMember States; \n• \nreduce political risk; \n• \nand increase the legitimacy of European integration.  \n  \nUnlike the punitive approach, the establishment of an ex-ante regulatory framework, adopting a \ncompliance approach that is focused on creating strong incentives to make the social network \nand its members accountable as well as strengthening the credibility of self-regulation, offers a \nunique opportunity to propose a change in Europe’s orientation.  \n \n2.5 \nA regulatory policy based on a compliance approach to be applied and designed with \npragmatism and agility \n \nIn the financial sector, governments have attempted to promote the credible and long-term commitment \nby financial institutions to actively contribute to achieving the public interest objectives of combating money \nlaundering, drug trafficking and the financing of terrorism. Banking supervisory authorities have therefore \ndevoted their efforts to imposing and monitoring obligations of means, i.e. compliance with certain \npreventive rules, rather than punishing failures when the risks being combated materialise (without prejudice \nto criminal proceedings in that case). Therefore, the banking supervisory authorities do not intervene when \nit is found that a financial institution has been the channel for channelling funds used for unlawful purposes, \nbut when it finds that a financial institution is not implementing a prescribed prevention measure, regardless \nof whether or not the financial institution is implicated in unlawful behaviour. This intervention approach \nis designed to create targeted incentives for platforms to participate in achieving a public interest objective \nwithout having a direct normative action on the service offered.  \n\n \n \n \n \np. 17 \nApplied to social networking services, this type of intervention involves identifying the few generic \nobligations likely to create the right incentives, particularly by increasing the effectiveness of political \ndialogue with Member States’ political institutions and their civil societies. This implies imposing strong \nobligations on the transparency of key systems unobservable from the outside, i.e. the moderation system \n(and procedures for developing and updating the terms of use that underlies it), as well as the use of \nalgorithms for targeting and personalising the content presented.  \nThis approach must be implemented progressively and pragmatically according to the size of the operators \nand their services: \n• \nOnly services with the most influence due to their size – and therefore the most dangerous in terms \nof their potential impact in the event of abusive use – should be subject to these obligations and to \nex ante compliance checks by the regulator. The regulator should focus mainly on active supervision \nof systemic actors; \n \n• \nMid-size services should be allowed an initial presumption of compliance and receive support from \nthe regulator, which could encourage their accountability commitment by issuing recommendations \nand implementing measures to increase pooling of common assets (e.g. database identifying \nunlawful content, access to annotated data sets for machine learning by moderation algorithms) \nwith the largest platforms, in an open, transparent approach resulting in lowering barriers to entry. \nNevertheless, if the regulator finds that a mid-size service is in clear breach of these obligations of \nmeans and that this results in excessive manifestation of the harmful effects being combated by the \npublic policy, the regulator must then be able to ask the operator providing that service to take \nappropriate measures and, if these measures are not implemented by the platform, also be able, in \na reasoned decision, to impose an enforceable ex ante compliance procedure on it, similar to that \nimposed on the most influential services; \n \n• \nFinally, concerning the smallest platforms, the regulator must be able to act only through dialogue \nand issuing recommendations, but without coercive action, without undermining its capacity to call \nthe prosecutor’s attention to any act that might be subject to criminal procedures.  \n \nAccountability mechanism deriving from  criminal and civil liabilities, particularly under the LCEN (French \nlaw to promote confidence in the digital economy), nevertheless remain applicable to all social networking \nservices, regardless of their size. These are not subject to the regulator’s action but rather to judicial bodies \nand common law procedures. \n \nThe legislative framework should allow gradual implementation of these mechanisms and \nrecognise the new regulatory system, as well as a capacity to define and gradually refine the \nobligations imposed, taking an agile approach in order to adapt quickly to changing social \nnetworks. In other words, the law establishing this regulatory framework must define the nature \nof the obligations, without seeking to define detailed procedures. Otherwise, the regulatory \nframework could easily be bypassed by “overly” agile operators. \n \n \n\n\n \n \n \n \np. 19 \nIII -  Organisation of the regulatory function in France  \n \nwithin a framework defined at the European level \n \nIt appears necessary to supplement the punitive measures against authors publishing unlawful content or \nseeking to manipulate social networks with a proactive dialogue approach, based on strengthening the \npolitical dialogue between the public authorities and the actors concerned. Creating the conditions for \nconstructive and regular dialogue with social network platforms should transform them in solutions’ \nprovider by encouraging them to adopt a responsible approach to their users and society and to prevent \nabusive use of their services. \nThis regulatory policy could be based on the following five pillars: \n \n \nFirst pillar  \nA public regulatory policy with broad objectives \nguaranteeing individual freedoms and entrepreneurial freedom  \nTo unite political energies both at national and European level and to bring together political institutions \nand civil society, the objectives of the regulatory system must be to defend the exercise of all rights and \nfreedoms on social media platforms: \no Individuals’ freedom of expression and communication, with individuals therefore being entitled to \nunderstand how the platform respects that freedom; \no Individual freedom of users to be protected in their physical and moral integrity, including on social \nnetworks in the digital space; \no Social networks’ entrepreneurial freedom, including the right to define and apply terms of use, to \nexercise an unrestricted information ordering system and to innovate (especially for smaller \noperators). \n \nThe objectives could also include secondary objectives of: \no pluralism of social network services and therefore a public policy position ensuring that new \nservices are supported and that no entry barriers are created; \no social cohesion, by encouraging the social networks to develop “positive” uses of their services, \ni.e. that strengthen social relations. \n \n\np. 20 \nSecond pillar  \nA prescriptive regulation \nfocusing on the accountability of the social networks, \nimplemented by an independent administrative authority \n \no A regulation with prescriptive action limited to the sole structuring platforms at the level of \neach Member State, with two thresholds:  \n \n• \nThe regulation would automatically apply for services for which the number of monthly users \nrises beyond a certain percentage of the population of the Member State (between 10% and \n20%).  \n \n• \nThe regulatory system would also be applied only following a reasoned decision by the regulator \nin the event of an identified and persistent breach for services with a monthly number of users \nlying between 0% and 5% of the population of the Member State. It should be noted that the \nlower this second application threshold, the more stringent and demanding the impact test \nmust be in order to comply with a general principle of proportionality.16 \n \n• \nThe regulation is not applicable below these thresholds, but the common law provisions of the \nLCEN remain in force, allowing action for civil and criminal liability of the operators in case \nof breaches.  \n  \no Transparency obligations that concern the key functions of the social networks17: \n \n• \nThe function of “ordering content”: that is to say, the methods of presentation, prioritisation \nand targeting of the content published by the users, including when they are promoted by the \nplatform or by a third party in return for remuneration; \n \n• \nThe system for implementing the terms of use and moderating content, including the \nmethods for the elaboration of these community rules, the procedures, the human and \ntechnological resources implemented to ensure compliance with these Terms and to fight \nagainst illegal content. This system must be able to be audited by the regulator and/or by an \nindependent auditor of the platform. Transparency can be seen in, for example:  \n� \nThe obligation to notify the platform's decision to the author of moderated content \n(except legitimate exceptions, e.g. if required by the public prosecutor) and the \nperson who flagged the content (where applicable); independent and extra-judicial \nmechanism for reviewing the platform's decision (without prejudice to a judicial \nremedy); \n� \nUse of automated processing tools: what tools are used, for what types of content, \nwith which human supervision? How is their effectiveness and accuracy assessed? \n� \nProcedures for cooperating with “trusted flaggers”: list, selection procedures, \n“privileges” attached to that status, statistical data on the number of reports \n                                                           \n16 This second threshold could be replaced by a “malfeasance” criterion in order to give the regulator the capacity to tackle any \nsocial networks raising issues. Nevertheless, the question arises of the appropriate level of the regulator’s resources and the necessity \nof creating a criterion for abandonment by the regulator in respect of small operators that no longer raise issues. \n17 The details of the transparency obligations set out below are given for illustrative purposes. They do not necessarily need to be \nincluded exhaustively in the text of the law defining the regulatory system and many may be subject to the regulatory discretion of \nthe government or the regulator. \n\n \n \n \n \np. 21 \nexamined, the number of contents detected proactively, the follow-up given \n(removal, maintenance, etc.), the appeals processed, etc.  \n� \nStatistics concerning moderation efficiency: decision times (whatever the decision \nmay be), false positive/negative rates, virality/audience of content contrary to \ncommunity standards before it was withdrawn (see concept of prevalence), etc.  \n \no A duty of care for social networks towards its members \n \nBy this obligation, which is close to the Anglo-American concept, the social networks social networks would \ncommit to be accountable for their users regarding abuses by other members and attempts to manipulate \nthe platform by third parties.  \nThe obligation of means would allow intervention by the public authorities if it appeared that platforms’ \napproach, currently voluntary, to ensuring that their users can have confidence, through the creation of \n“trust and safety” systems or the moderation system, lack resources. \n\np. 22 \nThird pillar \nBroad, informed political dialogue conducted transparently \nbetween the government, the regulator, the actors and civil society \no Using its regulatory power, the government sets the thresholds for triggering obligations and defines \nthe terms of the transparency obligations applicable to the functions of ordering content and \nimplementation of terms of use, as well as to the obligation to defend the integrity of the social network \nand its members. \n \no The government enact general regulatory decisions 18 made by the regulator. \n \no The government organises the political dialogue with social networks involving the regulator and civil \nsociety. \n \no The scope and effectiveness of political dialogue is enhanced by the regulator’s targeted actions aimed \nat promoting the social networks’ accountability. The government is then able to continue its action via \npolitical dialogue on all societal issues with the social networks by involving civil society (NGOs, regions \nand the educational and academic communities)19. \n \no Central agencies reposition themselves to support the government in its political dialogue by building \non the reduction of the information asymmetry between platforms and political institutions due to \nprescriptive regulations. \n \no Where applicable, the platforms make voluntary commitments to the government, which are subject to \nverification and enforcement by the regulator. For example, the implementation of an action plan to \ntackle a newly identified abuse, improvement of transparency metrics for the coming year, etc. \n \n \n \n                                                           \n18 Translation note: a general regulatory decision, “decision à caractère réglementaire” in French, is a decision setting rules \nthat apply to all actors as opposed to individual decision which apply only to a specifically designated actor. \n19 Specifically, the content of the terms of use remains within the social networks’ entrepreneurial freedom, although the \ntransparency provided by the regulator makes them subject to a political dialogue. For example, Facebook acknowledges that it has \nchanged the scope of its community regulations to better protect refugees from hate speech under political pressure. \n\n \n \n \n \np. 23 \nFourth pillar  \n An independent administrative authority,  \nacting in partnership with the other branches of the state,  \nand open to civil society \no In the regulatory system proposed, the independent administrative authority guarantees the \naccountability of social networks, for the benefit of the government and civil society. \n \no It implements coercive regulation autonomously but must not be self-sufficient or hegemonic. It would \nbe the regulator of the accountability of the large social network platforms by policing the transparency \nobligations of content ordering and moderation systems, as well as the duty of care towards its members. \nIt is neither the regulator of social networking services as a whole, nor the regulator of the contents that \nare published on them. It does not have jurisdiction over all contents taken individually. It cooperates \nwith other state agencies that are under the authority of the government and with the judicial services. \n \no It does not directly impose restrictions on the definition of the social networking services offered, but \nimposes the publication and dissemination of information, the veracity and relevance of which it seeks \nto qualify with the help of civil society (NGOs, regions and the educational and academic communities). \nIt verifies the effectiveness of the resources deployed to comply with the obligation to defend the \nintegrity of the social network and its members. \n \no It must have wide-ranging access to information held by the platforms, including the ability to use \nborrowed identities and to require special access to algorithms to verify the accuracy of the description \npublished by the social network20. It cannot be challenged on the grounds of business secrecy or \npersonal data protection, without undermining its obligation to protect the data it requires in accordance \nwith the GDPR and trade secrecy laws.  \n \no It has an administrative sanctioning power enabling it to impose (1) mandatory publicity on the social \nnetwork for these users and/or its commercial partners (i.e. the advertisers responsible for the \nplatform’s turnover), and (2) pecuniary sanctions up to a maximum of [4%] 21of the total global turnover \nof the social network operator. These sanctioning powers may be exercised only after formal notice. \n \no It has the mandate and legal competence to set up access links to enable academic research on the \nplatforms using their data, in compliance with GDPR. \n \no It has a mandate to encourage the pooling of resources and knowledge for the benefit of smaller social \nnetworks, thereby contributing to lowering entry barriers. \n \no It actively participates in the European regulators’ network and supports the government’s action in the \nnegotiation of European policy. \n                                                           \n20 Through the implementation of direct, real-time, targeted and proportional access to social network information systems via \ndedicated interfaces (APIs). \n21The mission has not conducted specific work on the level of pecuniary penalties to be set and uses the amounts provided by the \nGDPR.  \n\np. 24 \nFifth pillar  \nEuropean cooperation  \nwhich reinforces Member States’ capacity to deal with global platforms  \nand reduces the risks of implementation in each Member State \nIn view of the power of global platforms, the mission proposes that the European Union must organise \nthe networking capacity of governments and their civil societies by joining forces. This structure \nstrengthens Member States’ role as guarantors of social cohesion in a globalised world.  \nThis European level coordination must be based on:  \no A European direct regulation in order to recognise the global character intrinsic in any \ndigital platform; ensure the full effectiveness of coordinated and networked action by \nnational authorities in front of global players (in particular via common procedures, \ncommon APIs, etc.); and reduce the risks of implementation in each Member State. \no National implementation according to the destination country22 rule to make the platforms \nresponsible locally in each Member State and in the regions where they may create damage. \no Concerted action between national authorities and open to civil societies in order to \nincrease the effectiveness of verification of the platforms’ transparency. \no European mechanisms to reduce the risk of excessive regulation by a Member State \n(\"check and balance\"), an essential corollary of the competing competence of each \nMember State: national and European public consultation on regulatory decisions or \nrecommendations, a mechanism for referral of the opinions of the national personal data \nregulator, coordination and coherence of national regulatory decisions by a collegial body \nbringing together the national regulators and the European Commission. \n \n \n \n \n                                                           \n22 Member States’ competing jurisdiction should be limited by prerequisites: (1) a threshold expressed as the average number of \nmonthly users as a percentage of the population at national level to establish the regulator's automatic jurisdiction, and (2) a lower \nthreshold when combined with the finding of manifest harm in the destination country, and (3) limited power and penalties in \nproportion to the potential consequences suffered in the Member State. \n\n \n \n \n \np. 25 \nIV –  Focus on the transparency of algorithms \n \nIncreasingly complex algorithms \nUsers of social network experience algorithms every day, from content rating on the newsfeed, to insertion \nof sponsored content, algorithms to moderate content contrary to the terms of use, friend suggestions, etc. \nWhen dealing with the transparency of these algorithms, it is necessary to take the definition of the word \nalgorithm (“Set of operating rules whose application makes it possible to solve a stated problem using a \nlimited number of operations”), while also encompassing the algorithm's proposed input data, data that has \nbeen previously used to “train” the algorithm, the context data, etc. Many algorithms are based on statistical \napproaches and therefore provide probabilistic answers such as the probability that you might like content \nor click on a product. Some use machine learning techniques that involve trying to mimic human choices, \nfor example, the moderators’ choice to accept or reject content grouped into collections of annotated \ncontent. Most of them are also part highly personalised. Finally, the algorithms also evolve significantly over \ntime, sometimes updated daily. All of these factors lead to a paradigm shift for the intelligibility of \nalgorithms. Each algorithm may have billions of avatars that do not all behave in exactly the same way, \ndepending on the user or the country. Nevertheless, it is necessary for public action to extract the general \nprinciples. \nA need for algorithmic transparency \nAlgorithms are tools that may be misused or misappropriated. The importance they have gained on social \nnetworking platforms and the abuses they may cause (promotion of hate speech, ineffective moderation, \ninterference by a sovereign state in the public debate, etc.) have made state intervention vital. This involves \ntransparency, i.e. the means to make the underlying logic intelligible, the main processing principles applied \nby the algorithms. This first level, which requires little intervention from public authorities, allows a relative \nunveiling of the impenetrable workings of certain algorithms. it may also point to possible operating biases \n(whether due to developers’ conscious or unconscious choices, to computer programmes or to data bias), \nbut above all it fuels the public debate on the social questions raised by the widespread use of algorithms. \nIn real terms, algorithmic transparency takes a variety of forms. For example, for a private individual without \nany particular technical skills, it could mean publishing the key criteria that led to a result concerning him \nor her (information ranking, a recommendation, targeted advertising, etc.) or the reasons for a particular \ndecision (moderation of a post or lack of response following a report). A more expert operator will be \ninterested in more comprehensive measurements of algorithms’ performance (false positive or negative rate \nin moderation) or explanations of the processing architecture in the form of decision trees or other graphic \nrepresentations revealing the data taken into account by the algorithm and its influence on the results. The \nacademic world will surely be interested in the publication of reference datasets, making it possible to \nchallenge platforms’ moderation choices, without which it is impossible to reproduce the results of a \nlearning algorithm. \nTransparency cannot simply be declared. It is a particularly complex task to check the integrity of the \nalgorithms used by companies. The regulator must have the resources to do this using statistical measures, \nthe provision of API testing tools and third-party certification or compliance mechanisms. \n\np. 26 \nPublic action at two levels \nTransparency will be effective only if it results from regular dialogue with operators and a process of trial \nand error mimicking the development process of those algorithms. The key principles for action by the \npublic authorities therefore clearly need to be defined by legislation in order to impose transparency on the \nalgorithms, while giving flexibility to the regulator responsible for applying the law. The legislator must \ntherefore define the legal principles to be followed by the regulatory authorities while adapting to particular \ncontexts. In practice, it will particularly be necessary to establish a proper equilibrium between the principle \nof transparency and the protection of business secrecy and to define general obligations of intelligibility in \nrespect of the relevant algorithms. It would not be advisable, however, to define particular metrics in \nlegislation or to impose specific implementation procedures. For example, Article 14 of the law relating to \ncombating the manipulation of information is not understood by the sector, which finds it either excessively \nor insufficiently specific (what happens if the algorithm is customised? what happens if the algorithm is \nupdated? what does “share of direct access” mean in the context of a ranking algorithm?). \nSince the law has empowered the regulator to extract information, the regulator is able to study the specific \ncharacteristics of each algorithm on a case-by-case basis in order to ensure effective transparency. It is this \napproach – by definition experimental and involving co-construction and trial and error – that will allow \nthe regulator to develop its policy tools incrementally at a regulatory level. This could then subsequently \nlead to the emergence of possible synergies to define broader principles governing the transparency of \nalgorithms.  \nBeyond the technical and legal issues surrounding algorithms and their transparency, the aim of regulation \nwill be to bring the ethical issues and moral and political choices raised by algorithms into the public debate, \nto clearly reveal the “algorithmic policy”. The regulator's accumulated knowledge and the data it will collect \nwill then be able to enhance societal and academic debates better by responding to questions on the range \nof issues underlying the concept of transparency, for example regarding: \n• \nThe data:  \no What data is used to train the algorithms? How is it collected? Is it personal data? \no What data is used in the algorithm’s input parameters? \no Does this data present biases? \n• \nThe model:  \no What is the processing flow followed? What algorithmic components are used?  \no What supervisory/monitoring mechanisms are used in algorithmic learning? \no What personalisation is carried out? \no Does the algorithm reproduce biases? Can it be misused? What are potential abuses to \navoid? \n• \nInferences:  \no What are the false positive/false negative rates? \no Which metrics can be used to report on the algorithm’s performance? \no What confidence interval may be applied to the result? \no What procedures are used to correct errors?  \n  \n\n \n \n \n \np. 27 \n \nAppendix 1 \n \n \n \n \n \n \n \n \n \n \nMission letter \n\n\n \n \n \n \np. 29 \nMINISTRY OF THE \nECONOMY AND \nFINANCE \nSTATE SECRETARIAT \nFOR DIGITAL AFFAIRS \n \nMINISTRY OF ACTION \nAND PUBLIC ACCOUNTS \n \n \nMr Frédéric Potier \nPrefect on public service mission, \nDILCRAH \n \nMr Serge Abiteboul, \nInria researcher \nMember of the Arcep College \n \nParis, 11 March, 2019 \n \nRe.: Mission letter – Facebook mission \n \nDear Sirs, \n \nSocial networks now occupy an essential place in our society by offering their users powerful spaces and \ntools for exchanging ideas, discussing and sharing content. In this respect, they provide a fantastic \nopportunity to exercise freedom of expression and freedom of communication, the foundations of our \ndemocratic society.  \n \nWhile they are symbols of progress and spaces for freedom, social networks are also forums for the \ndissemination of unlawful content, which can be seen by a potentially very large audience. Freedom of \nexpression and freedom of communication, like other media, must be reconciled with other principles that \nmay limit the ability to exercise them, including respect for the dignity of the human person.  \n \nThe laws of the Republic obviously apply in the digital space, both in respect of users and platforms. \nHowever, the volume of content, the speed with which it spreads on social networks and its impact on \nsociety justify the complementary implementation of a systemic regulation of moderation systems on social \nnetworks. \n \nDeveloped by social networks on their own initiative, existing moderation tools adopted on a self-regulatory \nbasis which, while relevant in some respects, do not provide sufficient guarantees for the exercise of the \nfundamental rights of our fellow citizens.  \n \n \n \n \n \n \n \n \n \n \n \n139 Rue de Bercy - 75572 Paris Cedex 12 \n \n\np. 30 \n \nThese private initiatives can no longer manage without support from the public authorities. This was the \nconviction that led me to entrust you with a mission to investigate and make proposals concerning social \nnetworks’ content moderation systems. The lessons of this mission will complete the conclusions of the \nmission report entrusted by the Government to Ms Laetitia Avia, Mr Karim Amellal and Mr Gil Taieb.1  \n \nIt is in this context that Facebook, taking an experimental approach, has agreed to work with your fact-\nfinding mission and present its moderation system and its development prospects, with particular attention \nto combating the dissemination of content that incites hatred.  \n \nOn the basis of this unprecedented collaboration with a private operator, you will assess these self-regulation \nsystems adopted by Facebook. You will particularly study the algorithmic processing developed and used \nby Facebook in this respect.  \n \nYou will propose recommendations, whose risks and opportunities you will have first analysed, and in \nparticular the precautions to be considered in order to build a national regulatory framework which can be \ndeveloped on a larger scale, particularly in Europe, in view of the global nature of social networks. This \nexperiment should be seen as a first step to reflect enabling very specific consideration of the best ways to \nensure that all social networks, not just Facebook, apply very high standards and quality requirements in \nmoderating the content that they host.  \n \nTo carry out this mission, you will consult all stakeholders and take into account any initiatives already under \nway in other countries.  \n \nYou will draw on a team of experts whose composition is set out in the appendix and on three rapporteurs \nmade available for the duration of the mission. With your agreement, Mr Benoit Loutrel will be the general \nreporter.  \n \nYou will report periodically on the progress of your work to a steering committee made up of the offices of \nthe relevant ministers and chaired by my representative.  \n \nThe lessons learned from this experiment will feed into the national and European regulatory work in this \nfield. I wish to have your report available by 30 June 2019 at the latest.  \n \n \nMounir Mahjoubi  \n \n \n \n \n \n \n \n \n1 Report submitted to the prime minister on 20 September 2018, entitled “Strengthening the fight against racism and \nanti-Semitism on the internet”. \n \n \n\n \n \n \n \np. 31 \n \nAppendix 2  \n \nThe members of the mission \n \nSerge Abiteboul \nDirector of IT research at the Inria and the École Normale Supérieure in Paris, member of the Collège de \nl’Arcep, Serge Abiteboul is an expert in databases, information and knowledge. He was a professor (Collège \nde France, Stanford, Oxford University, etc.), a member of the CNNum, and a startupper. He is a member \nof France’s Academy of Sciences. He is also a blogger and author. \n \nFrédéric Potier \nAfter a career in the Ministry of the Interior and the ministerial cabinet, in May 2017 Frédéric Potier was \nappointed prefect on public service mission and interministerial delegate to the fight against racism, anti-\nSemitism and anti-LGBT hatred (DILCRAH). In this post, he guides the State’s public policy against certain \nforms of discrimination and hate speech and acts. \n \n \nCôme Berbain \nAs an engineer from the corps des Mines, and with a doctorate in cryptology, Côme Berbain is the State \ndirector for digital technology. His career has alternated between private entities (Orange, Trusted Logic) \nand public entities (Ministry of Defence, ANSSI) in the fields of digital transformation and cybersecurity. \nHe was an adviser to the office of the Secretary of State in charge of digital affairs in 2017 and 2018. \n \nJean-Baptiste Gourdin \nA graduate of the Paris Institute of Political Studies and of ENA, Jean-Baptiste Gourdin is department head \nand Deputy Director-General of the Directorate General of Media and Cultural Industries (DGMIC). He \nis a Magistrate at the Cour des comptes and was chief of staff of the CSA President and coordinator of the \nmission \"Act 2 of the cultural exception\". \n \nJacques Martinon \nA court magistrate, Jacques Martinon started his career as a trial judge. Since 2016, he has joined the \nDepartment of criminal affairs and pardons (DACG) and is head of the Justice Department’s mission to \nfight cybercrime. He is a contributor to the SGDSN’s cyberdefense strategy magazine, and has been a trainer \nfor the INHESJ/IHEDN joint national session on “digital sovereignty and cybersecurity”. \n \n\np. 32 \n \nGilles Schwoerer \nGilles Schwoerer is a gendarmerie officer and worked in a wide variety of State sectors (Army, Departmental \nGendarmerie, specialised gendarmeries related to aeronautics) before joining the Centre for combating \ndigital crime (C3N) in the Gendarmerie Judiciary Centre as Deputy Chief of C3N. \n \nAude Signourel \nAfter 7 years at the Ministry of Justice and 3 years in the cabinet of the prefect of Seine-Saint-Denis, Aude \nSignourel joined the Directorate of Civil Liberties and Legal Affairs at the Ministry of the Interior. Since \n2017, she has been legal advisor to the Cybercrime sub-directorate of the Central Directorate of the Judicial \nPolice, which hosts the PHAROS platform. \n \n \nThe mission rapporteurs \n \n \nSacha Desmaris \nSacha Desmaris graduated as a lawyer from the University of Paris 2 Pantheon-Assas, and is now is head of \nthe Audiovisual Economics Department at the Directorate of Studies, Economic Affairs and Forecasting \nat the Higher audiovisual council (CSA) where she was a policy officer from 2016 to 2019, after having \nworked four years at the general secretariat of the M6 Group. \n \nPierre Dubreuil \nPierre Dubreuil obtained his engineering degree from Telecom ParisTech and the École Normale \nSupérieure Paris-Saclay, and is a specialist in machine learning and co-founder of a startup; he is a policy \nofficer within the Electronic Communications and Postal Regulatory Authority (Arcep). \n \nBenoît Loutrel \nAn Inspector General of the INSEE, Benoit Loutrel specialised in industrial economics and regulation, and \nwas appointed Director General of the Electronic Communications and Postal Regulatory Authority \n(Arcep) from 2013 to 2016, after having worked there from 2004 to 2010. He was director of the \"digital\" \nprogramme on investments for the future from 2010 to 2013. He was director of public affairs for Google \nFrance for a few months in 2017. \n \n \n\n \n \n \n \np. 33 \n \n \n\np. 34 \n \n", "published_date": "2019-05-21", "section": "Dossiers"}